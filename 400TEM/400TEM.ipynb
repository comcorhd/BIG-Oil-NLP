{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ACDC-UD' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "! cd ../../; git clone https://github.com/alvelvis/ACDC-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../ACDC-UD')\n",
    "import estrutura_ud\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converter 400 TEM para utf-8\n",
    "Necessário ter a pasta do Petrolês em utf-16 (no repositório, a conversão para utf-8 já foi realizada, portanto é necessário pular esta etapa)\n",
    "\n",
    "Etapa necessária, caso contrário o UDPipe terá problemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir processado-utf8\n",
    "! mkdir gambiarra-utf8\n",
    "for file in os.listdir(\"TXT_Teses e Monografias\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(f\"TXT_Teses e Monografias/{file}\", encoding=\"utf-16\") as f:\n",
    "            with open(f\"processado-utf8/{file}\", 'w', encoding=\"utf-8\") as r:\n",
    "                r.write(f.read())\n",
    "                \n",
    "for file in os.listdir(\"Corpus Gambiarra\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(f\"Corpus Gambiarra/{file}\", encoding=\"utf-16\") as f:\n",
    "            with open(f\"gambiarra-utf8/{file}\", 'w', encoding=\"utf-8\") as r:\n",
    "                r.write(f.read())             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotar 400TEM\n",
    "--> DEMORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! ../udpipe/udpipe-1.2.0 --tokenize --tag --parse ../udpipe/bosqueud_2.5_workbench.udpipe processado-utf8/raw/*.txt > processado-utf8/udpipe/bem_processado.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UDPipe model: done.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! ../udpipe/udpipe-1.2.0 --tokenize --tag --parse ../udpipe/bosqueud_2.5_workbench.udpipe gambiarra-utf8/raw/*.txt > gambiarra-utf8/udpipe/gambiarra.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionar sent_id às frases das 400TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../scripts/fix_sent_id.py processado-utf8/udpipe/bem_processado.conllu > processado-utf8/udpipe/bem_processado_sentid.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ../scripts/fix_sent_id.py gambiarra-utf8/udpipe/gambiarra.conllu > gambiarra-utf8/udpipe/gambiarra_sentid.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrigir fórmulas nas 400TEM\n",
    "(ou só gambiarra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019922\n",
      "1019916\n"
     ]
    }
   ],
   "source": [
    "with open(\"gambiarra-utf8/udpipe/gambiarra_sentid.conllu\") as f:\n",
    "    gambiarra = f.read()\n",
    "\n",
    "gambiarra_split = gambiarra.split(\"\\n\\n\")\n",
    "print(len(gambiarra_split))\n",
    "gambiarra = [x for x in gambiarra_split if not \"\\t\\n\" in x and not re.search(r'\\n[^\\d#]', x)]\n",
    "print(len(gambiarra))\n",
    "\n",
    "with open(\"gambiarra-utf8/udpipe/gambiarra_sentid_formula.conllu\", 'w') as f:\n",
    "    f.write(\"\\n\\n\".join(gambiarra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir 400TEM UDPIPE\n",
    "--> DEMORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambiarra = estrutura_ud.Corpus()\n",
    "gambiarra.load(\"gambiarra-utf8/udpipe/gambiarra_sentid_formula.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bemprocessado = estrutura_ud.Corpus()\n",
    "bemprocessado.load(\"processado-utf8/udpipe/bem_processado_sentid.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de documentos em bemprocessado e gambiarra \n",
    "(estranho -- número diferente. Anotar novamente.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos em bemprocessado (403) que faltam em gambiarra (415):\n",
      "['343-20150401-TESEMSC_0']\n",
      "\n",
      "Documentos em gambiarra que faltam em bemprocessado:\n",
      "['1-20140905-TESEDSC_0', '140-20141209-TESEMSC_0', '18-20140905-TESEDSC_0', '255-20150506-MONOGRAFIA_0', '273-20140930-MONOGRAFIA_0', '29-20150519-TESEDSC_0', '37-20150226-TESEDSC_0', '43-20150519-MONOGRAFIA_0', '5-20140905-TESEDSC_0', '78-20140905-TESEDSC_0', '79-20140905-TESEDSC_0', '89-20141112-TESEDSC_0', '9-20140905-TESEDSC_0']\n"
     ]
    }
   ],
   "source": [
    "lista_documentos_bemprocessado = []\n",
    "[lista_documentos_bemprocessado.append(x.rsplit(\"-\", 1)[0]) for x in bemprocessado.sentences if x.rsplit(\"-\", 1)[0] not in lista_documentos_bemprocessado]\n",
    "lista_documentos_gambiarra = []\n",
    "[lista_documentos_gambiarra.append(x.rsplit(\"-\", 1)[0]) for x in gambiarra.sentences if x.rsplit(\"-\", 1)[0] not in lista_documentos_gambiarra]\n",
    "\n",
    "fora_de_gambiarra = [x for x in lista_documentos_bemprocessado if x not in lista_documentos_gambiarra]\n",
    "fora_de_bemprocessado = [x for x in lista_documentos_gambiarra if x not in lista_documentos_bemprocessado]\n",
    "print(f\"Documentos em bemprocessado ({len(lista_documentos_bemprocessado)}) que faltam em gambiarra ({len(lista_documentos_gambiarra)}):\")\n",
    "print(fora_de_gambiarra)\n",
    "print(\"\\nDocumentos em gambiarra que faltam em bemprocessado:\")\n",
    "print(fora_de_bemprocessado)\n",
    "lista_documentos_proibidos = fora_de_gambiarra + fora_de_bemprocessado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização dos documentos em ambos os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos proibidos:\n",
      "['343-20150401-TESEMSC_0', '1-20140905-TESEDSC_0', '140-20141209-TESEMSC_0', '18-20140905-TESEDSC_0', '255-20150506-MONOGRAFIA_0', '273-20140930-MONOGRAFIA_0', '29-20150519-TESEDSC_0', '37-20150226-TESEDSC_0', '43-20150519-MONOGRAFIA_0', '5-20140905-TESEDSC_0', '78-20140905-TESEDSC_0', '79-20140905-TESEDSC_0', '89-20141112-TESEDSC_0', '9-20140905-TESEDSC_0']\n",
      "\n",
      "Documentos em bemprocessado (402) que faltam em gambiarra (402):\n",
      "[]\n",
      "\n",
      "Documentos em gambiarra que faltam em bemprocessado:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documentos proibidos {(len(lista_documentos_proibidos))}:\")\n",
    "print(lista_documentos_proibidos)\n",
    "gambiarra.sentences = {x: y for x, y in gambiarra.sentences.items() if x.rsplit(\"-\", 1)[0] not in lista_documentos_proibidos}\n",
    "bemprocessado.sentences = {x: y for x, y in bemprocessado.sentences.items() if x.rsplit(\"-\", 1)[0] not in lista_documentos_proibidos}\n",
    "\n",
    "lista_documentos_bemprocessado = []\n",
    "[lista_documentos_bemprocessado.append(x.rsplit(\"-\", 1)[0]) for x in bemprocessado.sentences if x.rsplit(\"-\", 1)[0] not in lista_documentos_bemprocessado]\n",
    "lista_documentos_gambiarra = []\n",
    "[lista_documentos_gambiarra.append(x.rsplit(\"-\", 1)[0]) for x in gambiarra.sentences if x.rsplit(\"-\", 1)[0] not in lista_documentos_gambiarra]\n",
    "\n",
    "fora_de_gambiarra = [x for x in lista_documentos_bemprocessado if x not in lista_documentos_gambiarra]\n",
    "fora_de_bemprocessado = [x for x in lista_documentos_gambiarra if x not in lista_documentos_bemprocessado]\n",
    "print(f\"\\nDocumentos em bemprocessado ({len(lista_documentos_bemprocessado)}) que faltam em gambiarra ({len(lista_documentos_gambiarra)}):\")\n",
    "print(fora_de_gambiarra)\n",
    "print(\"\\nDocumentos em gambiarra que faltam em bemprocessado:\")\n",
    "print(fora_de_bemprocessado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambiarra.save('gambiarra-utf8/udpipe/gambiarra_sentid_formula_normalizado.conllu')\n",
    "bemprocessado.save('processado-utf8/udpipe/bemprocessado_sentid_normalizado.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anotar com jPTDP\n",
    "--> DEMORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.1).\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libdumbnet1\n",
      "Use 'sudo apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Collecting cython\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/d2/060a4f311c3b4a83cb050882f1032dabd5c8045b489dc699cff60bcebdba/Cython-0.29.14-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: cython, numpy\n",
      "Successfully installed cython-0.29.14 numpy-1.16.6\n",
      "Collecting dynet==2.0.3\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/de/181a8380e9fdb89d9aa5838059336bb535503d5f2053e621438e69081407/dyNET-2.0.3-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy (from dynet==2.0.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting cython (from dynet==2.0.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/d2/060a4f311c3b4a83cb050882f1032dabd5c8045b489dc699cff60bcebdba/Cython-0.29.14-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, cython, dynet\n",
      "Successfully installed cython-0.29.14 dynet-2.0.3 numpy-1.16.6\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/elvis/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-01-28 19:46:33--  https://www.dropbox.com/s/fn0r48xhn67inpt/outputs.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.1, 2620:100:601d:1::a27d:501\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/fn0r48xhn67inpt/outputs.tar.gz [following]\n",
      "--2020-01-28 19:46:42--  https://www.dropbox.com/s/raw/fn0r48xhn67inpt/outputs.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com/cd/0/inline/AxD2G4nj7yakMSjRE4y_YcQAHVKdbarXpH4iEcyhUN2SsS9UJKZ-_IUXgwKhu7BBiWejoM7s9HsxJJMys_XpWaJzQzjXdCnI1BR6_NUwa-MpsRQRzC-8dYHG6yoINcgz4n0/file# [following]\n",
      "--2020-01-28 19:46:42--  https://uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com/cd/0/inline/AxD2G4nj7yakMSjRE4y_YcQAHVKdbarXpH4iEcyhUN2SsS9UJKZ-_IUXgwKhu7BBiWejoM7s9HsxJJMys_XpWaJzQzjXdCnI1BR6_NUwa-MpsRQRzC-8dYHG6yoINcgz4n0/file\n",
      "Resolving uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com (uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com)... 162.125.5.6, 2620:100:601d:6::a27d:506\n",
      "Connecting to uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com (uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com)|162.125.5.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: /cd/0/inline2/AxC6Ef7c6VMjrTzJNe6xNSWrbEsDZDF6Ek9mEI9vYCvo5i2XY5SdFqinqhZE5GNprt5bKSBojn4aiDkhhxn0wYbN3DWcdaklGP1kUoQDoJfRMrIR2I0HxXV0-GeyFgwYMpEAhqTK6wyqkdV3HLU9DnLu6gXyVP36a74_mb0mY7ub9aygFNuCrCadxnTrNow36BV_uY8oSZf5utYmmLZp7CNlwZoivRiWDWOmm9MqT1JOY2OTph04LM-vfJ8c9q6ijGAd87YwSPPJkCBYb1LuBPdEp3uT1tdxwJ0-8DNNoQYZfLXeIglIOC_BS-5RN2Q-Rn5MICxzdUO7n7TuWhpIvLZT1zTbgxVeLx6pb5lrmGvpbw/file [following]\n",
      "--2020-01-28 19:46:52--  https://uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com/cd/0/inline2/AxC6Ef7c6VMjrTzJNe6xNSWrbEsDZDF6Ek9mEI9vYCvo5i2XY5SdFqinqhZE5GNprt5bKSBojn4aiDkhhxn0wYbN3DWcdaklGP1kUoQDoJfRMrIR2I0HxXV0-GeyFgwYMpEAhqTK6wyqkdV3HLU9DnLu6gXyVP36a74_mb0mY7ub9aygFNuCrCadxnTrNow36BV_uY8oSZf5utYmmLZp7CNlwZoivRiWDWOmm9MqT1JOY2OTph04LM-vfJ8c9q6ijGAd87YwSPPJkCBYb1LuBPdEp3uT1tdxwJ0-8DNNoQYZfLXeIglIOC_BS-5RN2Q-Rn5MICxzdUO7n7TuWhpIvLZT1zTbgxVeLx6pb5lrmGvpbw/file\n",
      "Reusing existing connection to uca0d830a5bc4a82dde796c2d491.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 136322224 (130M) [application/octet-stream]\n",
      "Saving to: ‘outputs.tar.gz’\n",
      "\n",
      "outputs.tar.gz      100%[===================>] 130.01M   653KB/s    in 3m 33s  \n",
      "\n",
      "2020-01-28 19:50:26 (625 KB/s) - ‘outputs.tar.gz’ saved [136322224/136322224]\n",
      "\n",
      "outputs/\n",
      "outputs/jPTDP_pt_ud.model\n",
      "outputs/jPTDP_pt_ud.params\n"
     ]
    }
   ],
   "source": [
    "! sudo -S apt install python-pip --yes < ~/sudopass\n",
    "! pip install cython numpy\n",
    "! pip install dynet==2.0.3\n",
    "! cd ../jPTDP; wget https://www.dropbox.com/s/fn0r48xhn67inpt/outputs.tar.gz; tar -xvzf outputs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dynet] random seed: 3378766694\n",
      "[dynet] allocating memory: 8000MB\n",
      "[dynet] memory allocation done.\n",
      "Loading pre-trained model\n",
      "Predicting POS tags and parsing dependencies\n"
     ]
    }
   ],
   "source": [
    "! python ../jPTDP/src/jPTDP.py --predict --model ../jPTDP/outputs/jPTDP_pt_ud.model --params ../jPTDP/outputs/jPTDP_pt_ud.params --test gambiarra-utf8/udpipe/gambiarra_sentid_formula_normalizado.conllu --outdir gambiarra-utf8/jptdp --output gambiarra_jptdp.conllu --dynet-mem 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dynet] random seed: 2794690350\n",
      "[dynet] allocating memory: 16000MB\n",
      "[dynet] memory allocation done.\n",
      "Loading pre-trained model\n",
      "Predicting POS tags and parsing dependencies\n"
     ]
    }
   ],
   "source": [
    "! python ../jPTDP/src/jPTDP.py --predict --model ../jPTDP/outputs/jPTDP_pt_ud.model --params ../jPTDP/outputs/jPTDP_pt_ud.params --test gambiarra-utf8/udpipe/gambiarra_sentid_formula_normalizado_4_fevereiro.conllu --outdir gambiarra-utf8/jptdp --output gambiarra_jptdp_4_fevereiro.conllu --dynet-mem 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dynet] random seed: 1083583026\n",
      "[dynet] allocating memory: 16000MB\n",
      "[dynet] memory allocation done.\n",
      "Loading pre-trained model\n",
      "Predicting POS tags and parsing dependencies\n"
     ]
    }
   ],
   "source": [
    "! python ../jPTDP/src/jPTDP.py --predict --model ../jPTDP/outputs/jPTDP_pt_ud.model --params ../jPTDP/outputs/jPTDP_pt_ud.params --test processado-utf8/udpipe/bemprocessado_sentid_normalizado.conllu --outdir processado-utf8/jptdp --output bemprocessado_jptdp.conllu --dynet-mem 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir 400TEM jPTDP\n",
    "--> DEMORADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambiarra = estrutura_ud.Corpus()\n",
    "gambiarra.load(\"gambiarra-utf8/jptdp/gambiarra_jptdp.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bemprocessado = estrutura_ud.Corpus()\n",
    "bemprocessado.load(\"processado-utf8/jptdp/bemprocessado_jptdp.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volte três casas: teste e normalização em jPTDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuição de DP e POS (ajustar onde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pos_dp_gambiarra = {}\n",
    "dic_pos_dp_bemprocessado = {}\n",
    "for sent, sentence in gambiarra.sentences.items():\n",
    "    for t, token in enumerate(sentence.tokens):\n",
    "        if not '-' in token.id:\n",
    "            dic_pos_dp_gambiarra[token.upos] = 1 if not token.upos in dic_pos_dp_gambiarra else dic_pos_dp_gambiarra[token.upos] + 1\n",
    "            dic_pos_dp_gambiarra[token.deprel] = 1 if not token.deprel in dic_pos_dp_gambiarra else dic_pos_dp_gambiarra[token.deprel] + 1\n",
    "for sent, sentence in bemprocessado.sentences.items():\n",
    "    for t, token in enumerate(sentence.tokens):\n",
    "        if not '-' in token.id:\n",
    "            dic_pos_dp_bemprocessado[token.upos] = 1 if not token.upos in dic_pos_dp_bemprocessado else dic_pos_dp_bemprocessado[token.upos] + 1\n",
    "            dic_pos_dp_bemprocessado[token.deprel] = 1 if not token.deprel in dic_pos_dp_bemprocessado else dic_pos_dp_bemprocessado[token.deprel] + 1\n",
    "\n",
    "print(f\"Classes em gambiarra ({len(dic_pos_dp_gambiarra)})\")\n",
    "print(sorted(dic_pos_dp_gambiarra))\n",
    "print(f\"\\nClasses em bemprocessado ({len(dic_pos_dp_bemprocessado)})\")\n",
    "print(sorted(dic_pos_dp_bemprocessado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Número de sentenças e tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = []\n",
    "[documentos.append(x.rsplit(\"-\", 1)[0]) for x in bemprocessado.sentences if x.rsplit(\"-\", 1)[0] not in documentos]\n",
    "print(f\"{len(documentos)} documentos foram encontrados em FINAL\")\n",
    "print(\"\\n\".join([\"* \" + x for x in documentos]))\n",
    "\n",
    "# a contagem de tokens não leva em consideração aqueles que têm \"-\" no seu id (indicação de contração (a contração em si conta, é claro))\n",
    "print(\"\\n|Versão|Sentenças|Tokens|\")\n",
    "print(\"|---|---|---|\")\n",
    "tokens_gambiarra = len([x for sentence in gambiarra.sentences.values() for x in sentence.tokens if not '-' in x.id])\n",
    "tokens_final = len([x for sentence in bemprocessado.sentences.values() for x in sentence.tokens if not '-' in x.id])\n",
    "print(\"|GAMBIARRA|{}|{}|\".format(len(gambiarra.sentences), tokens_gambiarra))\n",
    "print(\"|FINAL|{}|{}|\".format(len(bemprocessado.sentences), tokens_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuição das categorias bruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribuição das categorias bruta\")\n",
    "print(\"| {:25} | {:25} | {:25} |\".format('Anotação', 'GAMBIARRA', 'FINAL'))\n",
    "print(\"| {:25} | {:25} | {:25} |\".format(\"---\", \"---\", \"---\"))\n",
    "lista_pos_dp = [x for x in dic_pos_dp_gambiarra]\n",
    "[lista_pos_dp.append(x) for x in dic_pos_dp_bemprocessado if not x in lista_pos_dp]\n",
    "for pos_dp in sorted(lista_pos_dp):\n",
    "    print(\"| {:25} | {:25} | {:25} |\".format(pos_dp, dic_pos_dp_gambiarra[pos_dp] if pos_dp in dic_pos_dp_gambiarra else 0, dic_pos_dp_bemprocessado[pos_dp] if pos_dp in dic_pos_dp_bemprocessado else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuição das categorias relativa ao total de tokens da versão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribuição das categorias relativa ao total de tokens na versão\")\n",
    "print(\"| {:25} | {:25} | {:25} |\".format('Anotação', 'GAMBIARRA', 'FINAL'))\n",
    "print(\"| {:25} | {:25} | {:25} |\".format(\"---\", \"---\", \"---\"))\n",
    "lista_pos_dp = [x for x in dic_pos_dp_gambiarra]\n",
    "[lista_pos_dp.append(x) for x in dic_pos_dp_bemprocessado if not x in lista_pos_dp]\n",
    "for pos_dp in sorted(lista_pos_dp):\n",
    "    print(\"| {:25} | {:25} | {:25} |\".format(pos_dp, str((dic_pos_dp_gambiarra[pos_dp]/tokens_gambiarra)*100)+'%' if pos_dp in dic_pos_dp_gambiarra else 0, str((dic_pos_dp_bemprocessado[pos_dp]/tokens_final)*100)+'%' if pos_dp in dic_pos_dp_bemprocessado else 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização dos lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lemas_gambiarra = [lista_lemas_gambiarra.append(x.lemma) for y in gambiarra.sentences.values() for x in y.tokens if not '-' in x.id]\n",
    "lista_lemas_bemprocessado = [lista_lemas_bemprocessado.append(x.lemma) for y in bemprocessado.sentences.values() for x in y.tokens if not '-' in x.id]\n",
    "\n",
    "print(\"|{:25}|{:25}|{:25}|\".format('', 'GAMBIARRA', 'FINAL'))\n",
    "print(\"|{:25}|{:25}|{:25}|\".format(\"---\", \"---\", \"---\"))\n",
    "print(\"|{:25}|{:25}|{:25}|\".format('Lemas diferentes', len(list(dict.fromkeys(lista_lemas_gambiarra))), len(list(dict.fromkeys(lista_lemas_bemprocessado)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribuição dos lemas em GAMBIARRA\")\n",
    "print(\"|{:25}|{:25}|{:25}|\".format('LEMA', 'OCORRÊNCIA', 'REPRESENTATIVIDADE'))\n",
    "#print(\"|{:25}|{:25}|{:25}|\".format('', 'GAMBIARRA', 'FINAL'))\n",
    "[\n",
    "    print(\"|{:25}|{:25}|{:25}|\".format(\n",
    "            x, \n",
    "            lista_lemas_gambiarra.count(x), \n",
    "            str((lista_lemas_gambiarra.count(x)/len(lista_lemas_gambiarra))*100)+'%',\n",
    "        ))\n",
    "        for x in sorted(lista_lemas_gambiarra, reverse=True, key=lambda y: lista_lemas_gabiarra.count(y))\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
