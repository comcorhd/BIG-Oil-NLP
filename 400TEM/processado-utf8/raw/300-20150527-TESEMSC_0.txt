1. INTRODUCTION AND OBJECTIVES The continuous demand growth for liquid fuels, alongside with the decrease of fossil oil reserves, unavoidable in the long term, induces investigations for new energy sources. A possible solution to substitute some liquid fossil fuels is the use of bioethanol, produced from renewable sources (NAIK et al., 2010).
In Brazil, a massive production of ethanol as automotive fuel occurred in the 1970s, when the government initiated a national program (Pró-Álcool) to reduce the dependency on foreign refined oil. The Pró-Álcool program had as main starting material sugarcane juice. This culture was intensified during this period, especially in southeast Brazil. The technology used in the program, called first generation (1G), was similar to the one utilized nowadays. But two thirds of the cultivated biomass, i.e. sugarcane bagasse or other lignocellulosic materials such as leaves, generates non fermentable substrates when the current process is applied (FREITAS & KANEKO, 2012).
Even though the biomass is non fermentable via 1G processes may be used to generate other forms of energy (bioelectricity, production of syngas and so on), there is great interest in developing techniques capable of converting the carbohydrates from this material into bioethanol, thus generating more ethanol from each sugarcane mass unit (DANTAS et al., 2013).
Currently, large-scale 2G ethanol production still presents economic bottlenecks.
Among the most important technical hindrances is the scale up of the hydrolysis process to industrial application, in order to generate high product yields while keeping costs low (MODENBACH & NOKES, 2013).
This work intended to test and implement a softsensor architecture using Artificial Neural Networks, to predict the concentration of sugars in a bioreactor during the enzymatic hydrolysis of pretreated sugarcane bagasse using cellulases from a commercial cocktail. Besides, an algorithm based on optimal control theory was implemented, to define feeding strategies of substrate and/or enzymatic complex for the hydrolysis reactor.
It should be stressed that the main goal of this work is not to propose an optimal operational policy for a specific bioreactor, but rather to establish a consistent methodology to be applied in industrial plants. Besides, the experimental data that were used to fit models and to tune softsensors, and although the methodology may be consistent, the final results hardly could be applied, directly, to industry-scale reactors.
Nevertheless, the algorithms to be described here, using computational intelligence tools and applying advanced dynamic control theory, are expected, with modifications, to be useful for further use in the biorefinery, thus contributing for the consolidation of the 2G industrial process.
2. LITERATURE REVIEW 2.1. LIGNOCELLULOSIC COMPOUNDS Second generation biofuels are fuels produced from lignocellulosic substrates.
These are fibers found in plants and vegetables. Their main function is to provide structural support, while assuring microbiological and chemical protection. The fractions of lignocellulosic materials are mostly composed by cellulose (32–55%), hemicellulose (19– 24%), lignin (23–32%) and ashes (3–6%) (SANTOS et al., 2012).
Cellulose, (C6H1005)n, is the most abundant polysaccharide in the fiber. Its ordered structure consists of several hundred glucose molecules (XU et al., 2013). The cellulose spatial conformation is determined by three main interactions. The first interaction is the       glycosidic bond that unites a glucose to another glucose molecule through a covalent bond. This generates cellobiose, and this disaccharide is repeated throughout the polymer chain (ROCHA et al., 2011). The second interaction is among hydrogen molecules from the same chain and the third between adjacent chains. Due to these interactions, part of cellulose macromolecules can form a crystalline region, granting to the entire structure a high cohesiveness, and rendering it insoluble in water and several other solvents, and resistant to hydrolysis (SANTOS et al., 2012).
Hemicellulose differs significantly from cellulose. It is a heteropolysaccharide composed by hexoses (glucose, galactose and mannose), pentoses (xylose, the most abundant monomer, and arabinose), acetic acid, glucuronic acid and 4-O-methy- glucuronic acid. The relation between these substances differs from vegetable to vegetable. This portion of the tissue does not form crystalline regions, thus it can be removed or hydrolyzed more easily than cellulose (CANILHA et al., 2012).
Lignin is formed by the polymerization of p-coumaryl alcohol, sinapyl alcohol and coniferyl alcohol. It is the second most abundant polymerer in the lignocellulosic biomass, and provides a barrier against foreign agents. Recalcitrance of the biomass is in great part due to this lignin barrier (ARANTES & SADDLER, 2011). In order to prevent this effect, a delignification procedure may be applied. From a biorefinery point of view, the recuperated lignin may be used in other processes (STEWART, 2008). Alternatively, its combustion will provide an extra heat source to the 2G process.
To produce ethanol from these lignocellulosic materials the structural polysaccharides must be hydrolyzed, so that their monosaccharides (mostly pentoses and hexoses) become available to fermentative microorganisms. However, before the hydrolysis process, a pretreatment is required to separate and render cellulose and hemicellulose available to the hydrolytic action of the enzyme cocktail. A representation of the process is in Figure 1.
Figure 1 – Lignocellulosic Biomass Processing (Source: author’s collection, adapted from: SANTOS et al., 2012) 2.2. BIOMASS PRETREATMENT The pretreatment procedure destabilizes the lignocellulosic structure, making it more susceptible to further processing. This is achieved by increasing the material porosity, reducing cellulose crystallinity and removing lignin, to a certain degree. The entire procedure must be applied up to an intensity that generates an optimum platform for subsequent operations, while considering the formation of inhibitors and cost effectiveness (CHIARAMONTI et al, 2012).
Several methodologies are available for this process. Thus, the choice of the most adequate pretreatment depends on the feedstock, process plant design and economic situation (BANERJEE et al., 2010).
Among several available pretreatments, in this work special attention will be given to autohydrolysis, also know as hydrothermal pretreatment. This procedure requires a pressurized reactor to maintain water in a liquid state at temperatures ranging from 150ºC to 230ºC, for different time periods. At these temperatures, biomass suffers cooking, increasing cellulose digestibility, while producing small amounts of inhibitors (KIM et al., 2009).
Another advantage of this technology is the absence of additional chemical compounds during the process, yielding a less toxic effluent than other alternatives.
However, this pretreatment does not alters lignin to an extent that may render these molecules inactive in subsequent processes (MENON & RAO, 2012). A typical composition of sugarcane bagasse, for the most significant compounds, before and after the autohydrolysis process is presented in Table 1.
Table 1 – Typical Composition of Sugarcane Bagasse Before and After Autohydrolysis.
2.3. LIGNOCELLULOSIC BIOMASS HYDROLYSIS The pretreated biomass must be further hydrolyzed to provide fermentable fractions. At this stage, the polymers released by the pretreatment are converted to free monomers, readily available to fermentation. Two mains technologies are used in order to hydrolyze lignocellulosic materials, using acid solution, generally sulfuric acid, or using enzymatic complexes (GAMAGE et al., 2010; SUN & CHENG, 2002).
Acid hydrolysis is usually divided in two groups, diluted and concentrated acid hydrolysis. In diluted acid hydrolysis, acid concentrations range from 1 to 3% w.w -1, at high temperatures, 200–240ºC. Due to the high temperature toxic compounds and inhibitors, such as furfurals and hydroxymethylfurfural, are generated after the degradation of pentoses and hexoses . This degradation does not only decreases the hydrolysis final yield, but the generated compounds are toxic to further production stages, such as the fermentation process (LIMAYEM & RICKE, 2012).
Concentrated acid is a more common process methodology. In this operation the acid concentration is high and, therefore, temperatures can be lower. Thus, this process generates a smaller amount of inhibitors. However, the utilization of such high acid concentrations is costly, and also generates a highly toxic effluent (SUN & CHENG, 2002).
This leads to the necessity for a more economically and environmental suitable process. One alternative is enzymatic hydrolysis. This sort of procedure yields high conversions, with fewer risks of producing toxic secondary products (LIMAYEM & RICKE, 2012). However, a high cost is inherent to this process, since, the compound itself has a high value and, with the current technology, direct enzymatic complex reuse is not feasible when using a soluble enzymatic complex (DANTAS et al., 2013).
To introduce enzymatic hydrolysis into the ethanol production route several research routes are explored, among them: enzymatic complex improvement (KUPSKI et al., 2013); implementations of the second generation technology alongside the first generation process (FURLAN et al., 2012); utilizing high substrate loading; hydrolysis optimization modifying the feeding policy to the reactor in a fed-batch process (HODGE et al., 2009; CAVALCANTI-MONTAÑO et al., 2013).
2.3.1. Modeling Enzymatic Lignocellulosic Biomass Hydrolysis To optimize the bioreactor design and operational conditions it is necessary to understand the kinetics that commands the interaction of the lignocellulosic material and the enzymatic complex. This study is difficult since several effects are reported among substrate and catalyst (SOUSA JR et al., 2011). To cope with such complexity, a large number of models are proposed to elucidate this process behavior.
Different approaches are used to model the process. A summary of them is presented in Table 2.
Analysis of Table 2 indicates that the model choice must be based on the goal to be achieved. As the model's phenomenological foundation increases, so does its complexity, generating the necessity for more specific data. However, for some reactive systems it may be unpractical—or even unfeasible—to measure all significant effects, necessary to validate more complex models. For a macro analysis of an industrial process, these details may not even be significant at all. Thus an adequate tradeoff between available data and model complexity must be sought.
Table 2 – Classification of Cellulose Hydrolysis Kinetic Models (Adapted from Zhang & Lynd, 2004) A good equilibrium point in this tradeoff is the utilization of semi-mechanistic models. Models of this category reflect some—at least rough—understanding of the phenomena occurring in the system, though only using relative simple data, such as product concentration throughout time, for model fitting and validation. This class of model is generally applied for optimization and designing industrial reactors (CARVALHO et al., 2013).
The most popular semi-mechanistic models for enzymatic reactions are derivations of Michaelis-Menten’s (Michaelis & Menten, 1913). However, Michaelis-Menten model is based on mass action laws valid for substrates (and products) in the fluid (liquid, in this case) phase. This is not true for the specific case of lignocellulosic hydrolysis, since most of the substrate is solid. The excess substrate to enzyme condition ([S]>>[E]), necessary to the quasi-steady state condition, is also never achieved, since the fraction of cellulose available to hydrolysis is not high enough. This derives from the fact that hydrolysis occurs in a heterogeneous medium, and the reaction is occurring on the substrate surface, so the enzyme must first diffuse to the reactive site to be able to act (BANSAL et al., 2009).
Nevertheless, literature has shown that Michaelis-Menten models may be suitable to fit experimental results of the hydrolysis of lignocellulosic materials, despite the lack of physical-chemical background. Yet, in order to use this type of model some assumption regarding the substrate solid state must be established. Two options are available in the literature: using a pseudo-homogenous assumption for the solid substrate (Equation 1). Or using a modified form of the model, which assumes that the soluble enzyme attacks the solid substrate, but with negligible changes of the substrate initial concentration (Equation 2); the soluble enzyme has to absorb (and desorb) from the solid substrate. The concentration of enzyme absorbed on the substrate must be much smaller than the amount free in the medium ([E]>>[Eads]) (CARVALHO et al., 2013).
v= k⋅[ E]⋅[S] ( K m+[S]) Equation 1 v= k⋅[ E]⋅[S] ( K m+[E]) Where, v is the reaction rate, k is the enzyme turnover number, [ E] is the enzyme concentration, [S] is the substrate concentration and K m is the Michaelis- Menten half-saturation constant.
Equation 2 Both models are showed to be able of fitting hydrolysis data. However, they do not account for inhibitors present in the process. Bezerra & Dias (2004) showed that a pseudo-homogenous model with competitive inhibition by the product was the most suitable model in this case, as other effects that may reduce hydrolysis rates, such as nonproductive cellulase binding, enzyme jamming and enzyme deactivation were not so significant, according to these authors. Following this approach, Equation 1 can be modified into Equation 3.
Equation 3 The same consideration of competitive inhibition can be applied to Equation 2, to generate a modified Michaelis-Menten model with product inhibition.
Equation 4 2.3.2. Enzymatic Complex Due to the high complexity of the lignocellulosic material, the enzymatic catalyst used for biomass hydrolysis is not composed by only one active protein, but by a congress of several molecules, each interacting with a portion of the substrate.
The enzymes that interact with cellulose to produce glucose are denominated cellulases. Cellulases are enzymatic complexes that may be produced by fermentation of filamentous fungi from the genre Trichoderma, Aspergillus and Penicillum (WYMAN, 2003).
Cellulases are divided in three main groups. Endoglucanases (endo-1,4-β- glucanase) work in the amorphous region of the cellulose molecule and binds randomly, liberating reductive ends in the chain. Celobiohydrolases (exo-1,4-β-glucanase) act in the reducing and non-reducing ends of the chain, both e natural ones, and on the ones generated by Endoglucanases. The last group is composed by β-glucosidases and its function is to hydrolyze cellobiose into glucose (THONGEKKAEW et al, 2008). Other enzymes may be used as addictives to enhance the performance of the cocktail. Oxidases such as lytic polysaccharides monooxygenases are one example (HORN et al., 2012).
Residual hemicellulose, that was preserved in the solid substrate after the pre- treatment of the biomass, can be hydrolyzed by the action of a group of enzymes know as α-L- Hemicellulases.
arabinofuranosidase, among others (JORGENSEN et al., 2007).
β-xylanes and endoxylanases, them are: Among All these enzymes are commonly found in commercial cocktails, although the exact composition of such complexes is not disclosed.
2.4. HIGH SOLIDS ENZYMATIC HYDROLYSIS The so called C6 liquor, essentially glucose for further fermentation (with Saccharomyces cerevisiae), is the output of the enzymatic bioreactor, where cellulose hydrolysis occurs. For the economics of the overall processes, it is very important that glucose is yielded at high concentrations, thus reducing the amount of water in the solution. Ideally, for sugar cane mills, the target should be minimizing the energy demand by reaching glucose concentrations as close as possible to the sugarcane juice’s, aproximally 180 g.L ¹ (FERNANDES, 2003). Either if the C6 liquor is used in separate fermenters or if it is mixed with the sugarcane juice, a concentrated C6 liquor will reduce the demand of heating power in the global process (DIAS et al., 2012; FURLAN et al., 2012).
⁻ Thus, a high solid consistency (load of substrate within the reactor) is necessary, generating a more concentrated carbohydrate solution at the end of the process. As previously mentioned, a more concentrated final product would enable the addition of the 2G stream to the 1G’s, before the fermentation (HUMBIRD et al., 2010), without (or with minimal) demand of evaporators after the hydrolysis reactor. High-solids loadings also generate economical advantages since the operational volume will be lower than with low- solids operation, resulting in less energy to heat or cool the reactor. Disposal treatment costs would be lower too, due to the reduction of water usage (HODGE et al., 2009).
High solids processes are those where the ratio of solid material to aqueous phase is such that very little free liquid is present (HODGE et al., 2009). As water becomes sparse within the reactor two main issues arise. Water is first and foremost necessary in order to provide a medium in which the chemical reaction will take place. At high solids content, mass transfer becomes an issue, since the enzyme will be hindered to reaching its reactive site (MODENBACH & NOKES, 2013).
The second issue is the reactive medium apparent viscosity. Water dilutes the solids inside the reactor, effectively decreasing viscosity, and increasing the lubricity between the particles. A larger lubricity decreases the required shear rate to agitate the reactor. A smaller agitation necessity leads to smaller power consumption. Therefore, at high solids rates, reactor mixing becomes an issue, due to the high power demanded (KRISTENSEN et al., 2009). Thus, it would be interesting to have a reactor operational policy that would bypass such conditions.
2.5. BIOMASS HYDROLYSIS IN SEMI-CONTINUOUS OPERATIONS Biomass hydrolysis in fed-batch processes appears as a promising strategy since adverse conditions of a standard batch are avoided. A process policy where substrate is fed into the reactor continuously avoids the necessity of beginning the process with high solid loadings, facilitating the system homogenization. Furthermore, in fed-batch process, when compared to the same process done in a standard batch process, the conversion and productivity is higher, since smaller solid loadings diminish inhibitions, especially enzyme/substrate inhibitions (HODGE et al., 2009).
Studies for the optimization of fed-batch processes may begin by promoting batch hydrolyses under high-solids concentrations, when stirring and mixing in the tank may become a problem (HODGE et al., 2008). After the reactor model is consolidated, alterations are made in order to contemplate the feeding flow (HODGE et al., 2009).
Most of the published studies deal with spreading the initial substrate load evenly during the batch time, and do not propose optimum profiles (CHANDRA et al., 2011; GUPTA et al., 2012). This implementation does not optimize the system, and may not maximize its performance. Optimal control theory (dynamic optimization) may be applied to maximize productivity and minimize the utilization of enzymatic complex (CAVALCANTI- MONTAÑO et al., 2013). A summary of previous literature results in the subject is presented in Table 3.
Table 3 brings important points to the discussion. The work of Chandra et al. (2011) is the only presented research that does not demonstrate an improvement with the fed- batch system when compared with a standard batch. This work also is the only one that does not alters enzyme complex concentration throughout the process, indicating that there can be a relation between the enzyme feeding profile and fed-batch/batch improvement .
Another important characteristic is that none of the cited works considers the power demands for stirring within the reactor. The papers do not consider how the solids concentration will influence the reactor operation cost, and when the solid concentration is considered, the value is related to the reactor operational range, and not related to some index indicating the performance of the process. Nevertheless, as it will be shown further, it is imperative to consider the agitation power, and how it is changed by solids concentration, for the optimization of the process.
2.6. OPTIMAL CONTROL The optimal control problem (or dynamic optimization) consists in, basically, finding control variables optimum profiles (several decisions dynamics), control parameters or project variables values (static variables) and possibly the process final time that maximizes (or minimize) a performance scalar (objective function or cost function) (RIBEIRO & GIORDANO, 2005; RAMIREZ, 2004). The direct formulation of the optimal control problem is as follows (SRINIVASAN et al., 2003): Equation 5 Subjected to: Equation 6 Equation 7 Where J is the functional, or cost function, x(t) is the state variables vector, and x0 is the, usually known initial conditions, u(t ) is the control variable profile throughout time, g is the equality constraint vector, S is the inequality constraint vector and p are static decision variables.
There are several methods to calculate the optimum solution. The solution method varies with how the state and input variables are handled, and how the numerical solution is carried out. The functional presented in Equation 5 may be optimized in a direct approach, by using an optimization algorithm, or indirect approach, by using methods based in variational calculus such as the Pontryagin's Minimum Principle and the principle of optimality of Hamilton-Jacobi-Bellman (SRINIVASAN et al., 2003).
A simple method for solving the optimal control problem stated (Equations 5–8) is the sequential approach. In contrast to indirect approaches, in this direct method no analytical differentiation is needed and it is an adjoint-free computation, i.e. no adjoin variables (Lagrange multipliers) has to be calculate. In this method however, the control vector, u(t) must be parameterized using a finite set of parameters—the actual decision variables. Though this method is easy to implement, it tends to be slow, especially when inequality path constraints are included in the problem. (SRINIVASAN et al., 2003).
2.7. ENZYMATIC HYDROLYSIS FED-BATCH OPTIMAL CONTROL It should be stressed that the definition of the functional, or cost function, is a key step to have a well-posed optimal control problem. Defining reasonable criteria to evaluate the “optimality” of a specific solution in real life is probably the most challenging task for the process engineer that is implementing optimal control algorithms.
In the special case of lignocellulosic hydrolysis, some possible performance indexes are: productivity per enzyme mass, fermentable carbohydrates conversion, final carbohydrates concentration, some economical index related with the operational cost of the bioreactor with the selling price of bioethanol. The dynamic control variables may be the mass inflow of substrate and of enzymatic complex.
Most optimal control techniques presented in the literature for biomass hydrolysis are open loop, no method for information feedback is used. That is, the optimal policies are previously computed, assuming that the real process will not deviate greatly from the model predictions. Thus, disturbances originated from several sources, such as substrate composition variations or errors in secondary control systems, are not corrected by the control software (UPETRI, 2013). To consider these variations it is necessary to close the control loop, enabling the automatic update of control profiles based on the current state and future possibilities.
The feedback of data gives the control layer capabilities for dealing with uncertainties, not considered in the internal models. Previously determined kinetic parameters may then be re-estimated, and optimum trajectories of the system, recalculated. The feedback may generate a better performance when comparing to open optimization. However, a closed-loop control mesh may render the system more sensitive to external variations. This comes from the fact that, to maintain the system optimized, the controller distributes the error into the controlled values by intensifying activations densities. This effect generates a stress in the component, since more activations are necessary (NAGY & BRAATZ, 2004).
Commonly, the update of optimal profiles for fed-batch processes is translated into changes in the feeding streams to the reactor. This is especially the case when temperature is not an adequate variable to be manipulated, following profiles that change with time: certainly, this is not a desirable strategy for an enzymatic reactor, since the catalytic action of the enzymes is restrict to narrow ranges of temperature. Essentially, this kind of reactor will be operated isothermally, in a temperature where the enzyme activity is high, while thermal inactivation is not significant. Although, this approach may be improved (considering thermal profiles during operation of the reactor), in this work the reactor will be isothermal (the temperature closed loop feedback control runs in standalone feature, with a fixed set point).
Besides temperature, other secondary variables may be manipulated by the dynamic control algorithm. One example is the agitation.
2.8. ONLINE FERMENTABLE CARBOHYDRATES DETERMINATION In the hydrolysis of the cellulose fraction of the biomass, the desired products are free carbohydrates, especially glucose. Therefore, to optimize this compound final concentration, a methodology capable of predicting concentrations of this substance online is necessary. Several methods can be applied to quantify them, ranging from titration and colorimetric techniques to chromatographic analysis (SLUITER et al., 2010).
Nevertheless, these techniques are used in laboratory scale, demanding qualified operators and a relatively long time (DEMARTINI et al., 2011). Therefore, usual analytical techniques are not suitable to online monitoring, and more suitable methods must be developed, able to be used within an automated supervision/control framework.
A possible alternative to monitor fermentable carbohydrates is the utilization of soft sensing to infer the state of the system. In a softsensor, a set of measurements, obtained from different sensors, are the input for a model (usually empirical, black box) whose output is the inference of the variable of interest. Although extrapolation is not expected to be accurate with this kind of model, the accuracy and precision of the predictions must hold when a set of input values is not contained in the original experimental data (but is within the range of the data used for tuning the softsensor). Among the most popular soft sensing algorithms are: Principle Component Analysis (PCA) combined with Least Partial Square (LPS) regression; Artificial Neural Networks (ANN); and Neuro-Fuzzy systems (NF) (KADLEC et al., 2009).
Artificial Neural Networks (ANNs) are mathematical models inspired by the mechanism that the human brain uses to handle information. One important application of ANNs is patter recognition. Among several types of ANN architectures, the multilayer perceptrons (MLP) can be highlighted. In this architecture each artificial neuron is connected to all the neurons in the following layer, the input for each neuron is multiplied by a weight value and then it is introduced into a transfer function. The network composition is carried by a training stage using an optimization method to minimize the error between the model output and the experimental value, using independent test data sets (not applied in the tuning of the ANN parameters) to avoid overfitting. After training, the network may be applied to predict the value of the monitored variable from the primary, directly measurable, variables (DEHURI & CHO, 2009).
2.8.1. Torque Measurement Torque is an important variable when analyzing the rheometry of a solution or suspension. Usually, torque measurement is done off-line: a sample of the medium is loaded in a bench rheometer (EHRHARDT et al., 2010).
In studies that monitor rheometry throughout the enzymatic hydrolysis process, a clear decrease in the torque demanded to agitate the medium is observed when the solids in the reactor are hydrolyzed (SAMANIUK et al., 2011) However, these authors did not measure the torque online. Using a system capable of monitoring the torque throughout the process may enable solids monitoring, and this measured data can be used in the soft sensor.
2.8.2. Visible and Ultraviolet Spectroscopy An analytical online system, capable of analyzing the supernatant optical properties, alongside the hydrolysis reactor, can uncover new behaviors of the hydrolysis kinetics.
Specially the presence of inhibitors of the enzyme complex within the reactor may be detected.
This idea is supported by the fact that lignin absorbs electromagnetic radiation strongly in the ultraviolet region. Some methodologies use this characteristic in order to ascertain lignin contents in the biomass (NREL, 2008; GOUVEIA et al., 2009; KLINE et al., 2010).
Thus, an instrumentation capable of measuring lignin, as well as other possible analytes, can be feedback into the controller unit in order to generate the input of the soft sensor, and provide information (including inhibitors concentrations) to re-parametrize the kinetic model used by the optimal control algorithm.
2.8.3. Conductance/Capacitance Spectroscopy Conductance and Capacitance Spectroscopy (CCS) is based on the generation of alternating electrical fields in the media (inside the reactor, in our case). Thus, the CCS sensor is an in situ measuring device. Under certain frequencies, some groups of molecules are polarized. This polarization changes the dielectric constant of the medium.
This can be measured as variations in the conductance, the capacity of the medium to allow an electrical current to pass through itself, and capacitance, the capacity of a medium to store electrical charge (VOJINOVIC et al., 2006).
CCS has been recently reported for monitoring the hydrolysis of lignocellulosic material by Bryant et al., 2013 who observed a linear correlation between capacitance the contents of solids inside the reactor.
Therefore, an instrument of this sort can be used to aid the monitoring of the reactor, either as a standalone instrument or as a source of input signals to the soft sensor layer.
3. MATERIALS AND METHODS 3.1. ENZYMATIC HYDROLYSIS Bagasse was donated by Usina Ipiranga S/A (Descalvado, SP) in July 16th 2014, and it is the product of milling sugar cane used to extract the high carbohydrate content juice from this vegetable. The bagasse was then dried, to a humidity close to 5%, and freezed at – 7 ºC.
Batch and fed-batch enzymatic hydrolysis were performed utilizing hydrothermally pretreated sugar cane bagasse. The pretreatment was carried out in pressurized reactor, with a maximum pressure of 13 Bar, at 200 RPM. The reactor was loaded with 0.10 grams of dry bagasse per milliliter of reactor (10 % w.v ¹⁻ ) and then programmed to reach 195 ºC and hold this temperature for 10 min. The pretreated bagasse was then dried in kiln for 24h at 60 ºC. This methodology was predetermined with the research group.
⁻ The batch experiments were realized using 10 % w.v ¹ of dry pretreated bagasse suspended in 4.80 pH and 50 mM citrate buffer and 50 ºC (WANG et al., 2012). The hydrolysis was carried out in stirred vessel containing 3 L of reactive media in a 5 L container. The reactor was stirred at 470 RPM by a pair of Elephant Ear impellers, both equally distributed between the vessel bottom and the liquid surface. Temperature inside the reactor was maintained using a thermostatic water bath set to 50 ºC. The total batch time was 48 h and manual analysis were performed at 0.0, 0.5, 1.0, 2.0, 4.0, 6.0, 12.0, 24.0, 36.0 and 48.0 h.
The enzymatic complex used was Cellic Ctec 2® donated by Novozymes Latin America (Araucária, PR). In the batch experiments, 1.04 g (13.84 mL) were added, this mass is equivalent to a loading of 10 FPU.g Bagasse ¹, which is the operational load in studies within the research group. Filter Paper Unity (FPU) is the unit used in order to measure cellulase hydrolysis potential.
⁻ The fed-batch experiments were performed with similar conditions to those of batch experiments, 4.80 pH and 50 mM citrate buffer and 50ºC. However, the substrate and the enzymatic complex were not added in the beggining of the process, but, however, fed to the reactor following a feeding profile, presented in Table 4. The experiments lasted for 6h.
The substrate feeding was carried out with a solids concentration of 40% in the inlet flow.
The reactor initial volume was 3 L and was filled until 3.5 L.
Two experiments were performed for each strategy. Manual samples were taken 2 minutes before and 2 minutes after each feeding instant.
Table 4 – Fed-Batch Feeding Profile 3.2. MONITORING AND CONTROL SYSTEM This work proposes the dimensioning and construction of a system capable of monitoring, translating the data from a sensor array into a product concentration prediction, evaluating the reaction state and optimizing further activations to maximize the process efficiency. A schematic of how the system works is presented in Figure 2.
Further explanations on how the systems interact are contained in the following items.
Figure 2 - Monitoring and control system 3.3. EXPERIMENTAL APPARATUS The reactor where the hydrolysis happens possesses an instrumentation array with the purpose of monitoring the free glucose concentration inside the reactive media at any given time during the hydrolysis process. The sensors measurements are relayed to a server that decodes the information, converts to the interest variable unit of measurement when necessary and stores the data.
Figure 3 presents a scheme of the system sensors.
Figure 3 - Sensor array coupling 3.3.1. Torque Measurement The torque measurement was achieved using digital dynamometer coupled to the stirring shaft. The electric motor was above a ball bearing mount, thus the engine was free to roll in its own axle. By coupling a dynamometer perpendicularly to a rod fixated in the ball bearing a force was measured. This force is proportional to the amount of energy necessary to agitate the reactive media. To convert the straight force into stirring power, Equation 9 was used.
P=T⋅ω Equation 9 Where P, is the power necessary for the stirring motion, T is the torque measurement itself and ω is the axle angular velocity. Torque may be substituted by the variables in Equation 10.
Equation 10 Where F is the force provided by the dynamometer and L the distance of the dynamometer coupling to the center of the agitation axle. Further modifications are provided by Equation 11.
Equation 11 Where N is the rotation frequency, results in a simplification to convert the force measured by the dynamometer into stirring power presented in Equation 12.
P=2⋅π⋅F⋅L⋅N Equation 12 This instrument relays data through a serial connection to a server under a RS-232 protocol. The server receives this information through a universal serial bus (USB) port and handles the data in a software layer inside a Python console. This measurement was made at 20 seconds of the batch.
3.3.2. Supernatant Sampling and Scan The supernatant optical properties was measured by an analytical line once an hour. The supernatant sampling begins with the filtration of the reactive media by a pumice stone filter. The driving force for the filtration was provided by a peristaltic pump. Part of the filtrated supernatant, 0.2 mL, was destined to a dilution vessel. The dilution was accomplished by a series of valves and a peristaltic pump. The dilution line worked iteratively, adding 4.0 mL per iteration, and the sample dilution necessity was assessed by the last scan, updating itself automatically.
After the dilution, the prepared sample was injected into a flow cuvette inside the spectrophotometer. With the sample properly contained, 20 scans ranging from 190 to 10 nm was performed. This range comprehends the ultra violet and visible region of the electromagnetic spectrum. The data generated by the scans were transmitted to the server by a serial connection, under a RS-232 protocol, and the server received the information through a USB port and decoded by a software layer operating in a Python console.
At the end of the scans, dilution water was injected into the cuvette to clean it. After the cleaning period, only water was added to the cuvette and 5 scans were performed to establish a baseline for the next series of scans.
The automation of sampling system was accomplished by a physical computational device for data acquisition called Arduino. The Arduino board is an open hardware platform capable of generating electronic outputs or reading inputs in a standalone method or as a slave for a server (BANZI, 2009). The signals to change the controller states are generated by a software layer coded and run in a Python console.
3.3.3. Conductivity and Capacitance Measurement The conductivity and capacitance measurements was performed by a single probe connected to a preamplifier and transmission module Fogale Nanobiotech. The frequency used was 382 kHz.
3.3.4. Enzymatic Hydrolysis Monitoring Through Conductivity Small-scale studies were carried out to assess how the conductivity changes inside the hydrolysis media and evaluate this methodology as a tool to monitor hydrolysis inside the reactor before adding this instrumentation a larger reactor. Three small-scale batch experiments were conducted in a 500 mL reactor, with conditions similar to the large-scale experiments. Citrate buffer at 4.80 pH and 50 mM, 10% w.v ¹ dry bagasse, 50 ºC and adding 0.17 g of enzymatic complex. In these experiments, the probe relayed its data through a serial RS-232 connection and the decoding was achieved by the proprietary software (FOGALE NANOTECH BIOMASS+ v 1.0).
⁻ In the larger scales, the rest of the instrumentation was applied, however when using the capacitance/conductivity system inside the 3L reactor the acquired data by this probe was relayed to the server through a 4 to 20 mA connection. The signal was read by the data acquisition module Arduino Mega through two analog input ports, and then the data was relayed to the server via serial RS-232 connection.
3.4. CARBOHYDRATES DETERMINATION Glucose determination was carried out manually at the offline sampling periods described in the Item 3.1. The analysis itself was performed via glucose oxidase/peroxidase enzymatic determination kit (Doles; Goiânia, GO, Brazil) and High- Performance Liquid Chromatography (HPLC).
The samples were withdrawn from the reactor by filling a 2 mL vessel with reactive media. The container was centrifuged for 7 min at 10,000 RPM. 0.5 mL of the supernatant was combined with 0.1 mL of sodium hydroxide 0.2 N to maintain storage preservation.
HPLC was used to determinate glucose, xylose and cellobiose concentrations. The samples were analyzed in Shimadzu SCL-10A chromatograph using refraction index detector RID10-A, Animex HPX-87H Bio-rad, using as mobile phase sulfuric acid 5 mM at a flow of 0.6 mL.min ¹. The samples were compared to previously established standards (NREL, 2008).
⁻ Enzymatic kit analysis was used to check HPLC glucose concentration. The analysis occurred by combining 10 μmL of the prepared sample and 1 mL of the enzymatic analysis complex, incubating the mixture at 37ºC for 5 min and measuring the absorbance at 510 nm. The measured absorbance was compared to a standard curve of glucose determined with the kit in the same manner that the sample is analyzed. All analyses were performed in triplicates.
3.5. COMPOUNDS BALANCE AND MODEL FITTING In order to estimate parameters and optimize feeding strategies, a global mass balance for the reactor and each compound was developed.
Given that the reactor is presented in Figure 4.
Figure 4 – Reactor Schematics and Inflows (Source: Author's collection) For the the presented schematics, the global mass balance is as follows: Equation 14 ˙mI is the mass inflow (g.min ¹) of both substrate (solids) and enzymatic ⁻ Where complex.
The measurement of volumes and densities of the humid bagasse substrate is not straightforward. The substrate is a slurry of pretreated bagasse and water. Here, to emulate the substrate feed in an industrial process, the substrate was dried and citrate buffer (50 mM, pH 4.8) was added, in quantitative amounts (masses).
The reaction medium consists of solids (that will be liquefied) dispersed in a liquid phase. As a first approximation, the variation of density of the liquid in the reaction medium (due to the liquefaction of the solid substrate into soluble products) was neglected. In other words, the density of the liquid medium was assumed constant, and equal to the water (the density of the enzymatic complex was also assumed equal to water's). In order to simplify the model, we assume as control volume only the liquid phase inside the reactor.
Of course, these hypotheses can be refined, using empirical correlations for the apparent densities, but this approach will be left as a suggestion for the continuation of the present work. Thus the global mass balance becomes: Equation 15 [S]Inflow is the mass of dry solids divided by the water in the inflow (with 40 % of solids in the inflow feed stream, [S]Inflow becomes 666 gDry Solids.LWater ¹), ⁻ [E]Inflow is the concentration of total protein in the enzymatic complex in the inflow stream (in g.L ¹,⁻ FE is the mass inflow of enzyme (in g.min ¹), ⁻ ˙V (L.min ¹) is the volumetric reactor inflow ⁻ ⁻ the used value was 75.71 g.L ¹), and .
The substrate mass balance is presented in equation 3.
Equation 16 Where S is the solid substrate mass in the reactor, R is the rate of disappearance of ⁻ substrate (gSubstrate.min ¹). The balance can be described in terms of substrate concentration as follows.
Equation 17 ⁻ ⁻ ⁻ pseudo-stoichiometric ratio (cellulose content in the solids, 0.55 w.w ¹, multiplied by the hydrolysis stoichiometric ratio between glucose and cellulose, 1.10 wglucose.wcellulose ¹). This equation can be further modified: ⁻ Equation 18 Equation 19 The product mass balance follows: dP dt =+R Equation 20 Where P is the product mass inside the reactor. The balance can be described in terms of product concentration: Equation 21 ⁻ global mass balance (Equation 2), and modifying it, the product balance becomes: Equation 22 The enzyme mass balance follows: Equation 23 ⁻ after modifications and using the global mass balance as: Equation 24 The mass balance for the solids suspended in the reactor must consider the production of coproducts (xylose and cellobiose), besides glucose: Equation 25 Equation 26 Equations 15, 19, 22, 24 and 26 are the equations used in the following items as the compounds balance.
The stoichiometric rates were adjusted with the data provided by the chromatographic analysis. The xylose and cellobiose concentrations are estimated from a linear fitting of these compounds and the glucose concentration. This procedure is carried separately for the batch and fed-batch experiments.
To avoid inconsistencies in the numerical solving of the model, the feeding profile cannot be a discrete vector with punctual in certain time instants. Therefore, the vector was interpolated to a continuous function throughout the time domain. A representation of this interpolation is presented in Figure 5.
(Source: author’s collection) (Where: Discrete feedings (kg) at certain time instants were approximated to a continuous flow (kg.h ¹))⁻ In the Fig. 4, the blue circles represent the discrete values (optimized feeding vectors) and the blue solid line represents the generated continuous function. The interpolation algorithm behaved equally for the bagasse and enzymatic complex input profiles.
The numerical method used to integrate the differential system was a Runge-Kutta 4th order with variable step. Particle Swarm Optimization (PSO) algorithm was used to fit the parameters.
PSO minimizes the average quadratic error between the system output and the experimental value (cost function chosen) by generating a series of particles. These particles are scattered in a multidimensional space, with as many dimensions as there are parameters to be optimized, in this case a 3 dimensions space. A velocity in each dimension is attributed to the particle. Since the dimensions are the parameters to be optimized, one particle position is tested inside the model to evaluate its fitting. If the new fitting is better (smaller error) than a previous one found by the same particle (Personal Best) the new position is attributed as a new best particular position. The fitting value is also compared to a Global Best, which is the best value and position achieved by any particle. After the comparison stage, the particles velocities are updated to make the particles converge to the best global and individual fitting. To fully emulate a swarm, the velocities are also regulated by a Momentum parameter. The Momentum decreases with each iteration, simulating fatigue within the particles in a moment when they should be near the minimum value (KARIMI et al., 2012). An explanatory pseudocode containing the described numeric procedure is presented in Table 5.
Table 5 - Particle Swarm Optimization Pseudocode # Initialization Set Initial Parameters: Population Size, Number of Iterations, Initial Momentum, Velocity Actualization Parameters Generates the population with random positions and velocities Generates best global and particular values and positions Imports the experimental data for the error minimization and validation # Main Loop While: the Number of Iterations < Maximum Iterations OR Error < Tolerance: # Error Minimization – Network Optimization For - Each Individual in the Population: Checks the fitting for the particle this instant If - The present fitting is smaller than the personal best Then: This vector becomes the personal best (pbest) position and value End If If - The present fitting is smaller than the swarm's best Then: This vector becomes the global best (gbest) position and value End If # Convergence Improvement Updates the velocity according to the best values and social parameters Decreases the swarm's momentum by a fixed value End For If - The number of iterations is enough Then: Randomizes positions and velocities to reinitialized the swarm End If End For # Final Procedures Tests the experimental data against the system output During this study, the algorithms worked with a population of 10 particles and for 200 iterations. The social parameters (KENEDY & EBERHART, 2001) c1 and c2 responsible for the weighting in the velocities update were set to 2.00 and 2.10 respectively. The equation that updates the velocity is presented in Equation 27.
Equation 27 number between 0 and 1, pbest is the particle's velocity with the best fitting, gbest is the position that obtained the best fitting among all particle's and xk is the particle's current position. The position update is presented in Equation 28.
Equation 28 Where, xk+1 is the particle's next position and M the particle's momentum.
The momentum parameter was initially set to 0.99. However, after all the particle's velocities were updated, this parameter was decreased until it reached a value lower than 0.20, after this point the momentum was reinitialized to 0.99, and positions randomization were performed. This approach is necessary to relocate the swarm from a possible local minimal point.
After the optimization procedure ends, the confidence interval for each adjusted model is calculated. An approximate confidence region can be calculated using Equation 29 (HIMMELBLAU, 1970).
Equation 29 Where C.R. is the confidence region range, s^Y i is the standard error for each parameter, F1-α is the upper limit of the F-distribution, m is the number of parameters and n is the number of experimental data points.
The contour for the sum of squares surface can be calculated according Equation 30 Where ϕ is the squared error threshold for the region, if a parameters group has a squared error value higher than this value it is considered outside the confidence error and ϕmin is the squared error for the optimized parameter (HIMMELBLAU, 1970).
The entire procedure follows the dynamic demonstrated in the Figure 6.
Figure 6 - Model Fitting Flowchart (Source: author’s collection) 3.6. FED-BATCH OPTIMIZATION With the optimized models and its optimized parameters, Equations 15, 19, 22, 24 and 26 presented in item 3.5 were used to optimize the feeding strategy. The mass flow were subjected to an optimization method, where the profiles, both for substrate and enzymatic complex, changed at each iteration until an optimum bagasse and enzymatic complex addition is achieved.
The sequential approach and PSO algorithm, described in the previous item, were used to solve the optimal control problem. Therefore, the input flow had to be parameterized. Vectors with equal amount of points for bagasse and enzyme addition were created. The first value is the initial compound addition, and the other points are additions throughout the process.
For each evaluated feeding profiles, an integration of the fed-batch product balance is realized. The performance of the simulated feeding profile was evaluated by converting the final carbohydrate concentration into potential ethanol, via theoretical maximum stoichiometric coefficient, and subtracting from the revenue of selling this product the cost of the enzymatic complex and electrical power necessary to agitate the reactor. This value is divided by the total mass of bagasse added in the process in order to generate a revenue related to the added mass (US$.kgBagasse ¹). Hence the objective function becomes: ⁻ ⁻ The price for ethanol was 1.50 US$.kg ¹ (FURLAN et al. 2012), the evaluated cost of the accumulated enzymatic complex mass was 1.20 US$.kg ¹ (FURLAN et al., 2012) and the electrical power cost was 59.00 US$.Mwh ¹ (DIAS 2011). The solids fraction in the feeding flow was 0.40. The total times of fed-batch utilized in the optimization were; 360, 240, 144, 120, 96 and 48 h and feeding points were realized once an hour. The simulated reactor initial volume was 10 m³ and throughout the simulations no final reactor volume was applied.
⁻ ⁻ A representation of how the optimization works is presented in Figure 7.
During the optimization, a series of restriction may be applied, to generate more feasible solutions. The profiles were subjected to maximum mass addition and maximum substrate concentrations at any given time.
After the optimization reached its stopping criteria, a batch process was simulated where the accumulated bagasse mass and accumulated enzymatic complex from the optimum profile were added in the beginning of the process. The batch process revenue was calculate following the methodology of the fed-batch process. This was performed in order to evaluate the differences between the batch and fed-batch processes.
Figure 7 - Feeding profile optimization (Source: author’s collection) 3.6.1. Stirring Power A vital part of the process PI is the cost of energy in order to agitate the reactor. To estimate this cost a relation between the solids inside the reactor and the engine torque, and subsequent stirring power.
In order to achieve this relation, an empirical model was fitted between the stirring power acquired by monitoring system and the solids inside the reactor. However, solids concentration is not available experimentally. Thus, after the most accurate enzymatic velocity model is chosen, the model is adjusted to each batch experiment following the methodology presented in item 3.5. The balance of the solids output was used in the fitting of the empirical solids/stirring power model.
3.6.2. Hydrolysis Reactor Plant Equivalence At the end of each optimization cycle, when the optimum profile was achieved, an extrapolation was performed to determine the reactor size necessary to operate a second generation ethanol production plant.
⁻ The fictitious plant operated alongside a standard ethanol plant, milling 500 t.h ¹ of ⁻ sugarcane. This generates, approximately, 132 t.h ¹ of bagasse, 20% of this bagasse was assumed to be used to produce second generation ethanol. Thus, the 2G plant must be able to process 26.4 tbagasse.h ¹. To estimate the necessary reactor volume, or the volume sum of parallel reactors, the total processed bagasse was divided by the reactor final volume and process total time. This calculation is shown in Equation 32.
⁻ Equation 32 Where H.C. is the hydrolysis capacity of the process, mAccumulated Bagasse is the total accumulated bagasse throughout the process, tf is the process total time and Vf is the process volume at the final time.
This value was then multiplied by the necessary productivity (26.4 tbagasse.h ¹)⁻ resulting in the volume necessary to process at this rate.
3.7. NEURAL NETWORK ARCHITECTURE OPTIMIZATION Neural Network (NN) models were used to translate the data from dynamometer and conductivity/capacitive probe to glucose concentration. The NN models were implemented in software Matlab 2012 using the Neural Network Toolbox.
The NN inputs were originated in the data provided by the instrumentation and the reactor state during the high volume batch and fed-batch hydrolysis and were: stirring power per reactor litter, conductivity, capacitance, accumulated substrate feeding and reactor volume; and the network output was the glucose concentration from the chromatography analysis. However, there were too few glucose experimental data points to train the NN correctly. To improve the network inference, the best kinetic model was adjusted for each experiment and the model predicted values were used in the network optimization.
Cross validation approach was used to avoid overfitting issues (NELLES, 2001).
The sample universe was first randomized. The samples were then divided in 5 sets. To evaluate an architecture 4 sets were used while training the network (the current training group) and the unused set was used to validate (the validation group) the current training.
This approach was repeated until all the sets were used as validation set. The average of the standard error of training and the average of the standard error of validation were used to evaluate the architecture performance. A graphical representation of this procedure is presented in Figure 8.
Figure 8 – Cross Validation Procedure (Source: author’s collection) The architectures taken into account were multilayer perceptrons with one hidden layer, the numbers of neurons in the hidden layer were 1, through 15. And the evaluated transfer functions for the hidden layer and the sum layer are displayed in Figure 9. Each transfer function was evaluated both for the hidden layer and the sum layer.
A possible NN optimum architecture is achieved when the average standard error from the validation departs from the linear tendency of accompanying the average standard error from the training. When this happens, a possible interpretation is that the complexity of the networks has become larger than the necessary for the system. The networks starts to contemplate, in the pattern recognition, the noise from the samples disrupting the network inference (overfitting).
Therefore, the optimum architecture is when the errors are closely related (NELLES, 2001). An example of the behavior is presented in Figure 10. In the presented example, the point in which the validation error departs from the training error is at around 12 neurons in the hidden layer, thus demonstrating to be the optimum architecture for this hypothetical network.
Figure 9 - Evaluated transfer functions (Source: author’s collection, adapted from Matlab Neural Network Toolbox User's Guide) Figure 10 - Training and Validation Data set Error (Source: author’s collection, adapted from NELLES, 2001) 4. RESULTS AND DISCUSSION 4.1. CONDUCTIVITY MONITORING The experiments conducted in the 500 mL vessel provided the data presented in Figure 11.
Figure 11 - Conductance and glucose concentration during hydrolysis.
(Source: author’s collection) (Where: Error bars are s.d. of triplicate measurements.) A linear negative correlation between conductance and glucose concentration in the medium supernatant was observed (see Figure 10 B), slope of -31.82, intercepting point of 70.51 and determination coefficient of 0.91. Thus, conductance may be a feasible option to follow real-time hydrolysis kinetics within the reactor. This motivates further studies using CCS to monitor the process.
Thus the conductance/capacitance probe was installed in the 3 L reactor to continue the studies.
4.2. FULL ARRAY INSTRUMENTATION With the full array of sensors properly placed in the reactor, two experiments were conducted for each hydrolysis policy. The instrumentation data for agitation power, capacitance and conductance during the batch experiments are presented in Figure 12.
Figure 13 presents the same instrumentation data for the fed-batch experiments.
Figure 12 - Full Array Monitoring During Batch Experiments (Source: author’s collection) (Source: author’s collection) The capacitance/conductance probe and dynamometer were able to monitor the experiments throughout the process. However, the analytical line and supernatant UV/VIS scanning did not show a level of robustness necessary for the application.
The analytical line monitoring system initially was able to sample the reactive media, dilute the samples and communicate with the spectrophotometer. However, during long term experiments, the line had problems both in the software and hardware. At first, a problem occurred with the serial connections and analysis scheduling. After the solution of such problems, during the experiment the decrease in size of suspended solids particles generated a cake on the filter membrane, which introduced a pressure drop that the used pump was not able to overcome, disabling the analytical line. Nevertheless, examples of sample scans in time periods where the analytical line was operating are presented in Figure 14, for the batch experiments.
(Source: author’s collection) The data curves overlapping hinders a better analysis, thus Figure 15 presents an amplification of the range between 220 and 400 nm.
(Source: author’s collection) A peak is present around the region of 283 nm. This peak may be correlated with the amount of lignin in the solution, since the same region is used in order to estimate this compound concentration in lignocellulosic materials (NREL, 2008; GOUVEIA et al., 2009; KLINE et al., 2010). Another important characteristic of the data-set is that the peak intensity rises with time. This may demonstrate that lignin is being released from the lignocellulosic matrix. Since lignin is a interferent in the process, biding irreversibly to the enzymatic complex (ARANTES & SADDLER, 2011), this instrumentation may be used in order to re-parametrize the enzymatic velocity model during the process, in order to assess the necessity of addition of enzymatic complex.
The robustness of this instrumentation setup was not reliable during the whole process, and the data generated from this analytical line was not used any further.
Nonetheless this system may generate important data for the controller software. Thus, further studies will contemplate improvements in the analytical line in order to adequate it to the process.
4.3. BATCH AND FED-BATCH EXPERIMENTAL DATA The measurements, generated from the HPLC, for the experiments conducted in the 5 L reactor, are presented in Figure 16 for the batch runs and Figure 17 for the fed- batch policy.
Figure 16 – Experimental Data for Batch Experiments (Source: author’s collection) Figure 17 – Experimental Data for Fed-batch Experiments (Source: author’s collection) Since the concentrations of xylose and cellobiose are small, instead of fitting kinetic models for the production of these carbohydrates, the experimental data were used to adjust a pseudo-stoichiometric ratio between glucose and the xylose and cellobiose concentrations. Glucose concentration and the concentrations of xylose and cellobiose at the same time were correlated, as it can be seen in Figure 18, for the batch experiments, and in Figure 19, for the fed-batch experiments.
Figure 18 – Batch Experiments Co-products Linear Fitting (Source: author’s collection) Figure 19 – Batch Experiments Co-products Linear Fitting (Source: author’s collection) The parameters for each linear fitting is presented in Table 6.
Table 6 – Co-products Linear Fitting This is an empirical approach, since the ratio between these products is actually dictated by their rates of formation – which depend on the enzymatic kinetics of a complex system of reactions, on mass transfer resistances, on the deviation from ideal mixing in the bioreactor, among other phenomena. This is the reason for obtaining different slopes in the correlations for batch and fed-batch operation.
However, since the focus of this work is on implementing and testing automation algorithms, and taking into account that the linear correlations adhered very well to the experimental data, this simplification was adopted – implying that only one kinetic model had to be fitted, describing the formation of glucose.
4.4. PARAMETERS FITTING The models presented in Item 2.4.1. were adjusted to the experimental data presented in Item 4.3. The models and experimental data are presented in Figure 20 for the batch experiments and Figure 21 for the fed-batch experiments.
Figure 20 - Model fitting for Batch Experiments (Source: author’s collection) Homogeneous; MO – Modified; NI – Non Inhibit; PI – Product Inhibited) (Where: MM – Michaelis-Menten; PH – Pseudo Figure 21 – Model Fitting for Fed-batch Experiments The parameters, from the models are presented in Table 7.
The analysis of the Figures and Table show that the models containing product inhibition were able to fit the data in a satisfactory manner. The models without product inhibition did not present an adequate adherence. However, a lack of fitting is observed in the final stages of the hydrolysis, especially for the long term batch policies. This suggests that another inhibition phenomenon is occurring within the process.
To improve modeling at the final stages of the hydrolysis it is necessary to evaluate other causes for inhibition or deactivation, such as thermal denaturation, or the addition of other compounds into the balance that may interfere in the system dynamics.
Table 7 – Models Parameters with 95% confidence intervals Table 8 – Correlation Table for the Modified MM Model with Product Inhibition Figure 22 that demonstrates the confidence region at 95% confidence.
Figure 22 – Confidence Region for the Modified MM Model with Product Inhibition.
(Source: author’s collection) (Where: Blue dots are data points inside the 95% confidence region and red dot optimum parameters) The confidence interval topology indicates, once again, that the parameters are highly correlated, and so it is expected that the current state of the reactor will have important effect on the results of the online re-parametrization of the model. This behavior further demonstrates the necessity of software for online re-parametrization of the model, in order to predict the kinetics of the reaction more accurately.
Carvalho et al. (2013) fitted the same kinetic models, but their substrate was steam exploded delignified (4% NaOH) bagasse, and the enzyme was Accellerase 1500®. Their experiments were carried out with 6.54% of solids (w.w ¹), 5.7 FPU.g -1 and in batch runs only. The modified MM model with product inhibition was also the most suitable kinetic model, and their adjusted parameters were: k: 0.0033±7.10⁻⁴ min ¹, Km: ±0.87 g.L ¹. Furthermore, a high correlation among the 22.06±10.28 g.L ¹, Ki: 7.61 ⁻ Pretreated Bagasse ⁻ ⁻ ⁻ parameters was also found. A nonlinear confidence region with a topology very similar to the one obtained here was observed as well.
Although this model is based on very important simplifications of the problem, the comparison between the two optimized sets of parameters may suggest some conjectures: Carvalho et al. (2013) used Accelerase 1500®, thus a lower value for K and a higher value for Km (when compared to the ones obtained in this work, using Cellic Ctec 2®) is not an unexpected result, since Cellic Ctec 2® is a more recent commercial cocktail.
On the other hand, a higher Ki value in Carvalho et al. (2013) may be attributed to their bagasse preteatment, capable of diminishing lignin content (that provided a substrate with only 5.37 % w.w ¹ of lignin, while in this work the lignin content was approximately 25% w.w. ¹).⁻ ⁻ The modified Michaelis-Menten model with product inhibition, with the optimized parameters, was used in the following items to predict values at times not contemplated by experimental analysis or to predict variables where experimental data were not available.
4.5. STIRRING POWER/SOLIDS RELATION With the predicted values from the adjusted models to the experimental batch policy data, the solids concentration and stirring power scatter is presented in Figure 23.
Figure 23 – Stirring Power/Solids Concentration Scatter Plot (Source: author’s collection) ⁻ The figure presents two behaviors, with a threshold around 97 g.L ¹. Therefore, the fitting of only one empirical model to the entire range of concentrations may not generate the necessary adherence. Thus, the profile was divided into two regions, one before and one after the 97 g.L ¹ solids marker. Both regions were subjected to an exponential fitting, the Figure 24, and Figure 25.
⁻ ⁻ Figure 24 – Solids Above 97 g.L ¹ Region Exponential Fitting (Source: author’s collection) The adjusted models are presented in Table 9.
The analysis of both the figures and table presents that the fitting of the region with higher solids concentration is more accurate. However, this fitting improvement is achieved since the data is this region is more sparse and with less inherent noise.
Nevertheless, the models were employed in order to predict the power necessary to agitate a vessel in a given solids concentration.
Table 9 – Stirring Power Fitting Models These models were employed in predict the power necessary to agitate the vessel in a given concentration of solids. However, further studies must contemplate the utilization of more appropriate models, as well as include further assays to elucidate the relation among agitation power, solids concentration and reactor volume.
4.6. FEEDING PROFILE OPTIMIZATION The feeding profile optimization, as describe in Item 3.6. was done using as the model optimized in Item 4.4 to predict reactions rates, and both models presented in Item 4.5 to estimate the energy demand for stirring the vessel.
The optimizer was applied without any restrictions on the optimization/operational variables. A summary of the optimizations is presented in Table 10.
Table 10 – Unrestricted Feeding Policy cost is lower. This behavior is presented in Figure 26, that shows the profiles of the compounds in the reactor, as well as the accumulated feedings.
To maintain solids concentration at low values, a large volume of reactor, or reactors, is necessary to process the stream of bagasse described in Item 3.6.2, especially in long term processes, when the amount of bagasse added to the reactor increases considerably.
The simulations show that there is a clear trade-off between PI and the final product concentration. This occurs because longer processes enables a longer interaction between substrate and enzyme, decreasing solids concentration, and allowing more solids to be added. Since the addition of solids also dilutes the reactor, more enzyme is added in order to sustain the reaction velocity. All this generates a higher productivity, with a higher product final concentration.
However, more solids generates the necessity for more enzymatic complex, making the process more expensive. This is also true for the agitation power. As the process time increases so does the energy necessary to agitate the vessel. This however is not true for the 48h process, where the PI and product concentration are lower than the other processes. This is explained by the fact that in a process as short as the 48h there is not enough time to convert the substrate into product, generating a costly process and with small product concentration.
This tradeoff is important since a possible alternative in the utilization of the hydrolyzed lignocellulosic biomass is stream integration (MACRELLI et al., 2012). This is the process of combining the sugar rich currents from the sugarcane mill and the hydrolysis reactor product. However, both streams must have similar concentrations, so that one will not dilute the other. Since the sugarcane juice carbohydrate concentration is around 180 g.L ¹ (FURLAN et al., 2013), the hydrolyzed liquor must be concentrated through evaporation if glucose end concentrations are too far from this target. Evaporating the hydrolysis product increases the cost of the process (notice that this cost was not considered in the PI defined here). Thus, the PI of the 48h process may be diminished. On the other hand, the longer (and with smaller PI) process may need less energy to concentrate the liquor, possibly generating a more attractive situation.
⁻ It should be stressed that the approach used here for defining PI isolates the reactor from the rest of the process. The resulting solutions may actually be sub-optimal, but an algorithm for online optimal control most certainly will be working within this approach, when dealing with real case industrial applications. Of course, some concatenation with a plant-wide optimization is desirable. One way to improve the link between local and global optimization is improving the calculation of PI. For instance, energy costs for concentration of the reactor effluent may be correlated to the contents of glucose in this stream. These ideas will be left as a suggestion for future work.
Nevertheless, in the presented simulations, the amount of enzymatic complex used in the process is not feasible for a large scale process. Enzyme mass in relation to the substrate reaches values that would rule out this process, since a plant capable of producing such amount of enzymatic complex would be unfeasible. The batch processes, performed in this work, used in the composition of the velocity models used 10 FPU.gBagasse, and the optimizations reached values 30 times higher than this. This may be related to the fact that the optimizer needs a small concentration of solids in order to achieve favorable PI situations. Thus, a large amount of enzyme is added in order to decrease solids rapidly.
To diminish this effect, the same optimization sequence was repeated. But now the cost of energy to agitate the reactor was not taken into consideration when calculating the process PI. The new simulation summary is presented in Table 11.
Table 11 – Unrestricted Feeding Policy Without Stirring Cost Not having a cost limitation correlated with the solids concentrations also reduces the need for large operating volumes.
However, since there are no limitations on the stirring energy, solids concentration is let free to reach unfeasible regions. The profile presented in Figure 27 exemplifies this condition.
In this set of simulations solids concentration reached values higher than 30% w.v ¹. For the reactor design used in this work this is not a feasible situation. Solids deposition occurs at concentrations close to the agitation models threshold (97 g.L-1).
⁻ To overcome the solids concentration and enzyme loading problems, a final set of simulations were conducted. In these, the agitation power was accounted for in the same manner as in the first set of simulations. However, at this time a restriction on the enzyme addition was applied: the total accumulated enzyme could not surpass 50 FPU.gBagasse ¹.⁻ This represents 5 times the amount used in the experiments assays and was considered a feasible value for large scale processes.
A summary of the optimizations is presented in Table 12.
Table 12 – Feeding Policy With Enzyme Addition Restriction enzyme addition to an operational level. This is demonstrated in Figure 28.
(Source: author’s collection) The plant operational conditions for these simulations were similar to those without enzyme restriction, since the same behavior of maintaining low solids concentration was observed. The enzyme restriction would not greatly alter the reactor design, particularly when dealing with long term processes.
However, in these situations the final product concentration was lower than in the other data sets, especially in the short term operations. This is because the lower enzyme addition requires more time to hydrolyze the substrate. This solution would then require more energy for evaporation of the reactor outlet stream. In order to adequate this process to the 1G-2G carbohydrates stream combination either a longer operation is necessary or the final hydrolysis product must be concentrated.
Another possible solution is achieving higher solids concentrations without expanding a large amount of energy with agitation. To do that the reactor must be redesigned to work with higher solids concentrations.
One design option is the utilization of a Continuous Tubular Screw Reactor (CTSR).
This reactor uses a pressurized screw in order to move the bagasse from one end of the tube to the other. Meanwhile the solids are in contact with the enzymatic complex and hydrolysis occurs with lower energy consumption (TOMÁS et al., 1997).
The utilization of an alternative reactor design would promote a pre-hydrolysis at a solid consistency where the standard stirred tank would not be operational. After this first pre-hydrolysis, the more liquefied substrate can be directed to a standard reactor to finish the hydrolysis.
4.7. NEURAL NETWORK CALIBRATION The values generated by the optimized model were used to train an artificial neural network to predict glucose concentration within the reactor. The input data were conductance, capacitance, agitation power, accumulated solids and reactor volume. The network errors for each network architecture are presented in Figure 29 and 30.
Figure 29 – Artificial Neural Network Errors – Poslin and Logsig Architectures The majority of architectures presented, disregarding certain deviations, the behavior shown in Figure 9, except for the architectures containing a Pure Linear activation function in the hidden layer. The lowest average validation error occurred when the architecture contained a Tangent Sigmoid transfer function both in the hidden and sum layers, and with 11 neurons in the hidden layer. The addition of further neurons to this architecture generates an increase in the validation error, demonstrating that the optimum amount of neurons already occurred.
A scatter plot of the simulated glucose values and the ones predicted by the network is presented in Figure 31.
(Source: author’s collection) This figure demonstrates a linear relationship between the input values and the ones predicted by the ANN. The determination angular coefficient of the data is 0.994, demonstrating a strong correlation among the predicted and input values.
⁻ However, at high concentrations (larger than 12 g.L ¹) the dispersion becomes less stable. This may be explained by the fact that at these concentrations only batch data are available This interferes in the prediction in two ways. First, less data are available in the region, since the fed-batch experiments did not reached these concentrations of product.
Second, these data are gathered at the final hours of the batch process, when the model used to generate the data becomes less adequate to describe the process, generating a deviation between the available data and the actual data.
Nevertheless, the network demonstrated to be a promising tool in order to estimate the state of the reactor online, doing so regardless of the policy used in the reactor (batch or fed-batch). This network, or a variation of it, may be used in order to estimate the reaction kinetics in real time, and thus, generate feedback information to the controller software, enabling a closed-loop control strategy.
5. CONCLUSIONS The first conclusion of this work is that the presented models were able to describe the hydrolysis to some extent. The best model was a Modified Michaelis-Mentem with product inhibition. A departure between the experimental and predicted values arises in long term operations. This may be due to inhibition effects not described by the model.
Thus, further inhibition studies need to be conducted so that the model may be more accurate.
The ANN, after the architecture optimization, was capable of predicting product concentration from available data with a strong correlation (Determination Coefficient of 0.972). Therefore, the softsensor can be tested in further studies to generate feedback of data to the dynamic control software, in a closed loop architecture.
The optimization software was able to generate profiles that increased the process performance index while maintaining operational levels within the reactor, reaching glucose concentrations close to those utilized in current first generation technology even when a restriction to enzyme feed was applied (156.0 g.L ¹ of glucose after 360h) or not (168.3 g.L ¹). However, using a stirred tank, fed-batch reactor in the industrial case would demand a, relatively, large plant (reactors ranging from 490 to 35,000 m³). Thus, reactor design must improved in order to adequate this technology to industrial use.
⁻ ⁻ 6. FURTHER STUDIES SUGGESTIONS Further study suggestions include: - Performing experiments to evaluate enzymatic complex inhibition during the hydrolysis.
- Performing validation experiments with feeding strategies generated by the feeding optimizer.
- Improve the optical analysis line robustness.
- Further studies with CSS in order to monitor the hydrolysis process.
- Generating a more complete model that correlates solids concentration with agitation power.
- Optimizing the ANN further, while using other tools in order to estimate reaction kinetics more accurately.
- Coupling a CTSR to the standard reactor to diminish the overall cost of the process.
