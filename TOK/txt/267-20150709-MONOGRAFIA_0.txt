I.
INTRODUÇÃO
A população mundial cresceu vertiginosamente a partir da revolução industrial, e associado a este crescimento o tamanho e o número de indústrias ampliaram de forma incontrolável, de modo que a demanda energética para manter esse sistema funcionando tomou proporções gigantescas. Se toda a energia que está sendo exigida for proveniente de combustíveis fósseis, a poluição, e consequentemente, os problemas ambientais irão crescer de forma descontrolada. Em 2012 apenas 21% do total da energia elétrica mundial foi gerada a partir de fontes renováveis, e a demanda global de energia foi de 82% de combustíveis fósseis. A expectativa é que a geração de energia elétrica a partir de fontes renováveis cresça em 45% entre 2013 e 2020. (FRANÇA, 2015, p.10) Segundo a Agência Internacional de Energia (AIE), em 2014 a geração de energia elétrica a partir de fontes renováveis cresceu 7% (350TWh) e contribuiu com 22% da geração mundial. A China ficou em primeiro lugar entre os países que possuem a maior participação de fontes renováveis na geração de energia elétrica (23% do total). (FRANÇA, 2015, p.20) A previsão para 2025 encontra-se apresentada na Figura 1.
Existem previsões de reduções no consumo de energia a serem alcançadas em 2025 relativas à trajetória atual, como redução de 13% no consumo de energia pela indústria, 25% de redução das emissões por transporte e redução de 80% da carga térmica de novas construções com a criação de construções mais eficientes energeticamente. (FRANÇA, 2015,) A contribuição na redução das emissões por setor é apresentada na Figura 2.
No Brasil, a previsão da demanda total de eletricidade é apresentada na apresentado na Figura 4.

A oferta interna de energia elétrica tem 70,6% originária de hidrelétricas e as
Figure 5: Oferta interna de energia elétrica por fonte em 2013 no Brasil Já a oferta interna geral de energia possui como principal fonte combustíveis fósseis como petróleo (39,3%), gás natural (12,8%), dentre outras apresentadas na Figura 6.
Atualmente, tem-se notado com grande evidência as consequências do uso exacerbado de energia de fontes fósseis (petróleo, carvão mineral) como: agravamento do efeito estufa, causado pela emissão de gases poluentes (CO2, NOx e SOx, dentre outros) com consequente elevação da temperatura média da Terra (porém vale ressaltar que ainda há contradições na comunidade acadêmica quanto às reais causas do agravamento deste problema), e aumento da concentração de particulados em suspensão na atmosfera. Devido a isso o planeta se encontra em um estado de alerta cujo foco é o desenvolvimento de combustíveis de fontes renováveis, além de desenvolvimento de estudos para a fixação do CO2 presente na atmosfera de modo a minimizar sua concentração atmosférica.
Essa necessidade pela busca por fontes alternativas de energia, no Brasil, surgiu na década de 70 com a Primeira Crise do Petróleo, quando a necessidade de substituição das importações levou o governo a instituir o Pró-álcool, o que contribuiu para um grande desenvolvimento do setor sucroalcooleiro, principalmente na década de 90 quando o governo retirou os subsídios, provocando uma real modernização do setor para competir no âmbito internacional, e levou também à geração de milhões de empregos.
Durante muitos anos o Brasil liderou o mercado de produção de biocombustíveis com o etanol a partir da cana-de-açúcar. Porém hoje, devido ao fornecimento de subsídios do Estado americano, esse mercado é liderado pelos Estados Unidos, que produzem etanol a partir do amido de milho, e o Brasil se encontra em segundo lugar. (NOVACANA.COM, 2015) A produção brasileira é referência mundial na produção de bioetanol, visto que o processo de produção é autossuficiente energeticamente, pois por volta de 75% bagaço da cana gerado após a moagem é queimado para gerar energia para o processo (CANILHA et al.,2007), e ainda há sobra de energia que é vendida. A produtividade do etanol proveniente da cana-de-açúcar é superior à do milho, sendo 7000-8000 litros de etanol por hectare e 3500 litros por hectare, respectivamente.
(WEINGRILL, 2007) A produção do etanol via cana-de-açúcar consiste na fermentação do caldo extraído da planta na moagem, com a participação da levedura Saccharomyces cerevisiae.
Estudos estão sendo feitos para a produção de etanol a partir do bagaço excedente do processo (etanol de segunda geração), que é um material lignocelulósico composto por cerca de 50% de celulose, 30% de hemicelulose e 20% de lignina. Se a instalação desse processo for economicamente viável, será possível ampliar a produção deste biocombustível sem haver a necessidade de ampliar a área plantada.
O biodiesel é outro biocombustível que está sendo industrialmente produzido, e tem tomado uma posição de importância neste setor. Ele é produzido a partir da transesterificação de óleos vegetais e animais a partir da hidrólise básica, ácida ou enzimática (ARANDA, 2007). A produção desse biocombustível pela rota enzimática está sendo muito estudada devido às vantagens de produzir produtos com maiores teores de pureza, além de permitir o uso de matérias-primas com maior teor de ácidos graxos livres, exigir condições brandas de temperatura e pressão e não formar subprodutos. (ABREU, 2013) Em vista do apresentado anteriormente, os processos de produção dos biocombustíveis abordados são processos cujo comportamento cinético são difíceis de serem modelados, por se tratarem de processos biológicos, nos quais características do sistema, como propriedades específicas do meio e dos organismos presentes, afetam drasticamente o comportamento do sistema. Como consequência, existe uma grande deficiência nas ferramentas de estimação de parâmetros de processos biotecnológicos, pois elas exigem do usuário um conhecimento aprofundado sobre os métodos de otimização e sobre a linguagem de programação utilizada. Tais ferramentas são de grande importância na área de bioprocessos, pois os modelos cinéticos existentes que descrevem com maior acurácia o comportamento experimental são, em geral, complexos o que dificulta a estimação dos parâmetros cinéticos sem o auxílio de uma ferramenta computacional. Por outro lado, pesquisadores e alunos que trabalham com bioprocessos, realizando cultivos celulares, fermentações e avaliando influência de condições operacionais na produtividade, em geral não são familiarizados com linguagens de programação, métodos matemáticos e de otimização. Percebe-se, portanto, uma lacuna, por uma ferramenta de estimação de parâmetros de modelos de bioprocessos que seja de fácil interação para usuários não familiarizados com os métodos.
Este trabalho visou desenvolver uma ferramenta que fornecesse uma maior interação com o usuário. O código foi feito de modo que a leitura dos dados de entrada (inseridos pelo usuário) fosse feita dinamicamente, ou seja, o programa lê apenas as informações fornecidas. O usuário tem a autonomia de decidir quantos pontos experimentais deseja inserir bem como os valores dos parâmetros do algoritmo de otimização.
Ademais, a elaboração desta ferramenta visou auxiliar estudos em desenvolvimento na área de produção de biocombustíveis, bem como ser suporte para a otimização desses processos, visto que uma ferramenta computacional eficiente e de fácil compreensão é de grande auxílio nesta área de bioprocessos.

II. OBJETIVOS
No sentido do exposto anteriormente, o trabalho visou a implementação de ferramenta de otimização que permitisse ajustar os parâmetros de modelos que descrevam a produção de biocombustíveis por meio de processos fermentativos.
Assim, esse trabalho de Iniciação Científica possuiu por objetivo geral implementar a ferramenta de otimização, aprimorar os conhecimentos em métodos da Engenharia Química e no uso de programas especializados de programação e métodos de otimização, bem como na metodologia do desenvolvimento científico e tecnológico. A aplicação em processos fermentativos de produção de biocombustíveis alia o desenvolvimento em área onde há carência de ferramentas amigáveis específicas (processos biotecnológicos) a inserção em temática de grande relevância para o desenvolvimento tecnológico.

III. REVISÃO BIBLIOGRÁFICA
Além do etanol, outros biocombustíveis também estão sendo estudados com o objetivo de contribuírem para uma matriz energética mais sustentável, como o biobutanol produzido pela fermentação ABE, biometano produzido a partir da biodigestão da biomassa, dentre outros.
Mariano et al. (2008) desenvolveram um trabalho de produção alternativa do butanol, utilizando o processo de fermentação ABE associado a um destilador flash.
Este estudo foi desenvolvido através de modelagem matemática e simulação computacional. Esse método alternativo foi estudado, pois um dos problemas na produção tradicional do butanol em um reator de batelada quando comparada à produção petroquímica são os produtos intermediários e a inibição por produto que ocorre a uma concentração de aproximadamente 13 g butanol/L, levando a uma baixa concentração final de butanol.O estudo analisou a viabilidade técnica da produção de butanol a partir da tecnologia de fermentação contínua associada ao flash. Apesar do butanol ser menos volátil que a água, a termodinâmica garante sua remoção da mistura. A produtividade ABE obtida foi consideravelmente maior do que em processos sem a remoção do solvente, com concentração final de 28,2 g butanol/L.
Tendo em vista a importância que os biocombustíveis receberam nos últimos anos, fica clara a necessidade do desenvolvimento da modelagem dos processos biotecnológicos, visando a otimização dos mesmos. Mais do que isso, existe uma enorme deficiência de programas simples para simulação e determinação de parâmetros nos bioprocessos, o que incentiva o desenvolvimento de ferramentas mais simplificadas.
Segundo Edgar e Himmelblau (1988), um modelo matemático é “uma representação dos aspectos essenciais de um sistema existente, que representa o conhecimento sobre aquele sistema de forma útil”.
A modelagem matemática de bioprocessos envolve os balanços de massa para cada componente presente no bioreator, de modo que pode ser utilizada para prever o comportamento dinâmico e estacionário do processo, auxiliando no projeto de algoritmos de controle e na otimização dos processos.

A modelagem é de extrema importância porque é uma abstração que ajuda a evitar experimentações e observações repetitivas, economizando tempo e dinheiro.
Porém, no desenvolvimento de um modelo não há a possibilidade de representar exatamente o sistema real. Adaptações e análises quanto aos fatores a serem considerados devem ser feitas de modo a possibilitar o desenvolvimento do mesmo.
Logo, ao se fazer uso de uma modelagem deve-se ter conhecimento de todas as considerações embutidas no modelo para que não se cometam erros significantes.
Um modelo pode ser classificado primeiramente em dois grandes grupos: baseado na teoria física ou podem ser estritamente descrições empíricas. Ele pode ainda se subdividir em: linear, não linear, descrever processos em estado estacionário ou não estacionário, com variáveis contínuas ou discretas, com parâmetros concentrados ou distribuídos. E quanto mais um modelo se torna complexo, maior a dificuldade dos métodos de otimização a serem empregados.
(EDGAR e HIMMELBLAU, 1988) Os parâmetros de um modelo são coeficientes que devem ser ajustados de modo a diminuir o máximo possível a diferença entre os dados obtidos experimentalmente e os fornecidos pelo modelo. Com isso, tem-se um novo conceito a ser abordado: a otimização.
A otimização, dentro da perspectiva da estimação de parâmetros, consiste em minimizar uma função objetivo que reflete o desvio entre os dados experimentais e os calculados pelo modelo, de modo a obter o conjunto ótimo de parâmetros. Os métodos de otimização podem ser classificados em diretos, nos quais se faz uso apenas dos valores da função objetivo, iniciando-se com um valor para x e calculando valores sucessivos para a f(x) para outros valores de x; e em indiretos, no qual o cálculo de um potencial extremo é feito usando derivadas da função objetivo e/ou das restrições do problema. Os métodos indiretos têm a vantagem de convergirem mais rapidamente, porém em engenharia essa vantagem é neutralizada pelo fato de que dados extremamente precisos não são de tanto interesse na determinação de um extremo, visto que há uma grande incerteza quanto aos coeficientes da função objetivo. Já os métodos numéricos diretos têm a vantagem de tratar mais facilmente funções com descontinuidades. (EDGAR e HIMMELBLAU, 1988) Em Edgard e Himmelblau (1988), diversos métodos de otimização aplicados à engenharia química são apresentados, desde a otimização de funções de uma variável, a funções multivariadas, não lineares restritas e irrestritas.
Muitos dos algoritmos para otimização restrita (em que existem restrições para a otimização da função objetivo) e irrestrita fazem uso de técnicas de otimização unidimensionais eficientes quando a função objetivo é de uma variável. E apesar desse procedimento não ter elevada eficiência no encontro do ótimo, resultados aceitáveis são obtidos. Porém, para otimizar funções multivariadas, o tempo computacional pode ser um fator limitante para a aplicação desta técnica.
Ao selecionar um método de otimização, a complexidade do procedimento, a robustez e a eficiência são pontos a serem avaliados cuidadosamente de modo que a técnica seja bem aplicada para encontrar a solução com o menor erro possível.
Nos métodos de busca unidimensional para funções de uma variável, um bom ponto de partida é necessário para que a otimização seja eficiente. E isso é possível a partir da experiência prévia do usuário no assunto. Alguns desses métodos são: método de Newton, diferenças finitas e o método das secantes. Além desses, ainda existem os métodos de aproximação polinomial (interpolação quadrática e cúbica), onde um ponto x perto de x* (ótimo) é localizado por extrapolação e interpolação usando aproximações polinomiais como modelos para a f(x).
A otimização numérica de funções objetivo multivariáveis não lineares gerais exige que técnicas robustas e eficientes sejam utilizadas porque estes problemas requerem o uso de processos iterativos de solução. Estes processos são geralmente “divididos” em duas fases: a escolha da direção de busca, e a minimização completa ou parcial para encontrar um novo ponto.
Os métodos não lineares de otimização se diferem principalmente no modo como eles geram as direções de busca. Dentre eles, têm-se os métodos diretos de busca, como busca aleatorizada, busca Grid, busca univariada, método simplex, busca por direções conjugadas e o método Powell, que são muito simples de executar e entender além de serem satisfatórios para problemas de duas variáveis.
E além desses tem-se os indiretos de primeira ordem que utilizam derivadas de primeira ordem para determinar a direção de busca, podendo-se citar dois:  método gradiente: usa apenas a primeira derivada da função objetivo nos cálculos, e faz a busca na direção negativa do gradiente (para problemas de minimização). Porém não se trata de um método muito eficiente pois tem uma convergência lenta e ocorrem muitas oscilações.  método gradiente conjugado: combina informações correntes sobre o vetor gradiente com aqueles vetores gradientes de iterações prévias para obter a nova direção de busca.
Já os métodos indiretos de segunda ordem fazem uso de uma aproximação quadrática da f(x), sendo possível levar em consideração a curvatura da função na hora de determinar a direção de busca. Um deles é o método de Newton, considerado o método mais eficiente quando bem sucedido. Porém, apresenta algumas desvantagens: não encontra necessariamente o mínimo global, requer uma inversão de matrizes e exige a primeira e a segunda derivadas da função objetivo, o que pode inviabilizar processo, ou torná-lo muito custoso computacionalmente.
Muitas vezes, um meio de evitar o cálculo das duas primeiras derivadas da função objetivo, como citado acima, é aproximar essas derivadas por diferenças finitas, o que, segundo Edgar e Himmelblau (1988), pode ser aplicado à otimização irrestrita sem perda significante de precisão, porém, com maior gasto de tempo.
Umas das técnicas de otimização mais utilizadas e mais efetivas é a programação linear, em que o termo ”linear” se refere a processos em que a função objetivo e as restrições são lineares. Para exemplificar algumas dessas restrições encontradas em uma indústria química tem-se: limitações de estocagem, limitações de produção devido a restrições de rendimento dos equipamentos, limitações do mercado, dentre outras.
Um procedimento analítico geral para lidar com problemas de programação linear de grandes dimensões é o procedimento iterativo chamado de algoritmo Simplex.
Este algoritmo possui a vantagem de fornecer muito facilmente informações de sensibilidade, cuja análise consiste em calcular a mudança do valor da solução ótima de acordo com mudanças nos coeficientes da função objetivo ou nas restrições. Esta análise fornece, na prática, por exemplo, informações sobre como as mudanças nos preços das matérias-primas ou de venda influenciam no ponto ótimo, o que é de grande valia para a indústria.
Quando, em uma otimização, os problemas em questão possuem funções objetivo e restrições lineares, o método Simplex e suas variações se adaptam bem às exigências. Porém, quando o assunto é uma função objetivo não linear sujeita a restrições lineares ou não lineares, outros métodos de otimização, mais robustos, devem ser aplicados, haja vista que um dos problemas mais simples de programação não linear é a programação quadrática.
Dentre eles, o método do multiplicador de Lagrange é o mais clássico. Ele é de grande importância na interpretação da otimização, principalmente na análise da sensibilidade de parâmetros em problemas com múltiplas restrições. Porém, para problemas gerais este método não apresenta grande eficiência quando comparado a outros como os métodos lagrangianos aumentados, programação linear sucessiva e a quadrática sucessiva. Este último é considerado um dos métodos mais recentes e talvez um dos melhores para resolver problemas de programação não linear.
Quando se fala em métodos de otimização não se pode esquecer um dos primeiros propostos: o método de busca randômica. Porém, foram feitos testes com vários algoritmos de busca aleatória que confirmaram a baixa eficiência destes quando comparados aos métodos determinísticos já citados.
Segundo Jeronymo et al. (2010), a Otimização por Enxame de Partículas (PSO) foi criada por Kennedy e Eberhart em 19951 baseado nos estudos do sociobiologista Edward Osborne Wilson. É um método que pertence à família de Algoritmo Evolutivo, e portanto, trata-se de um método de otimização estocástico, ou seja, se baseia no sorteio de números aleatórios ao longo da execução da otimização, e não necessita de gradientes da função objetivo, o que é bom pois em muitas aplicações obter o gradiente está associado a um alto custo computacional.
Ele representa o comportamento de grupos de pássaros percorrendo um espaço, de modo aparentemente aleatório. Cada partícula é uma solução, em potencial, no espaço, para o problema de otimização. A cada iteração, de acordo com a sua melhor posição (mais próxima do mínimo, se for um processo de minimização, ou
Kennedy, J. e Eberhart, R. (1975). Particle Swarm Optimization. In: Proceedings of the IEEE International Conference on Neural Networks Piscataway, NJ, pp. 1942 – 1948. mais próxima do máximo, se for de maximização) e a melhor encontrada pelo enxame, a velocidade de cada partícula do enxame é alterada de modo a atingir melhores resultados. (JERONYMO et al., 2010). As equações que descrevem o cálculo da nova posição e da velocidade descritos anteriormente são, respectivamente, as Equações 1 e 2.
Onde: ( ) : novas coordenadas para cada partícula na iteração k+1; ( ) : nova velocidade para a iteração k+1; ( ): coordenadas do melhor local que cada partícula já esteve a iteração em que se encontra; ( ): coordenadas do melhor global que todas as partículas já estiveram até iteração em que se encontra; i: contador do número de variáveis; j: contador do número de partículas; ( ): fator peso, descrito na Equação 3

O PSO é um método de otimização que possui uma vasta gama de aplicações, principalmente onde os Algoritmos Genéticos podem ser aplicados.
Dentre as aplicações, tem-se treinamento de redes neurais, ajuste de sistemas de interferência para monitoração de sensores, projeto de núcleo de reatores nucleares, análise de sistemas financeiros (MENESES, 2010).
Os processos fermentativos se diferem dos processos químicos em geral em alguns pontos: apresentam baixa concentração, baixa velocidade de reação, conhecimento insuficiente dos fenômenos limitantes, falta de sensores “on line” para a ampliação de escala do processo e para implementação de estratégias de controle multivariável nos bioreatores, além da complexidade do sistema reacional.

Todos estes pontos apresentados dificultam muito a modelagem desse tipo de processo, que se divide em: modelos fenomenológicos e modelos empíricos; quanto ao grau de descrição da população microbiana, os modelos podem ser: modelo estruturado (devido à dificuldade de modelar fielmente um processo biológico, o uso deste tipo de modelo, em geral, é dificultado), não estruturado, segregado e não segregado. (PEREIRA, sem ano especificado) As equações cinéticas são de enorme importância para a modelagem fenomenológica, porém, algumas simplificações na representação desses processos fermentativos devem ser feitas para que a modelagem seja possível: controlar o biorreator quanto ao pH, temperatura e oxigênio dissolvido. Na marioria dos casos,a menos de um componente, os outros não são limitantes para a reação. As equações cinéticas, portanto, devem representar adequadamente os fenômenos que interferem no crescimento celular e na formação do produto, como as inibições.
Existem vários modelos empregados na determinação da cinética de reações fermentativas, um deles é o modelo de Monod, representado pela Equação 4
(SHULER e KARGI, 2002).
Onde: = velocidade específica de crescimento 
() = velocidade específica máxima de crescimento 
() = constante de saturação (g/l) = concentração de substrato (g/l) Existem modelos que consideram o aumento da concentração de inibidores no meio reacional, como alta concentração de substrato, produtos e componentes tóxicos, que podem exercer uma relação de competição pela enzima, reduzindo a velocidade da reação.

O etanol, não pode ser produzido apenas através da cana-de-açúcar, beterraba ou milho, mas também por meio da fermentação de alguns resíduos industriais. O soro de leite, por exemplo, é um resíduo produzido em grande escala nas indústrias de queijo e apresenta potencial para produção de bioetanol. Este efluente, devido à elevada concentração de lactose e carga orgânica, é considerado um grande poluente ambiental quando liberado no meio sem passar por um pré-tratamento. O soro de leite representa 85-90% do volume de leite utilizado na fabricação de queijos (SILVA et al., 2010), e a cada quilo de queijo produzido, restam em torno de nove litros de soro (formado por aproximadamente 95% de água, 4% de lactose e 1% de proteína). Em 2003, a produção de soro no Brasil ultrapassou os 4,3 bilhões de litros anuais. (SUGIMOTO et al. 2006).
Apesar da grande produção de soro de leite no país, o Brasil se encontra entre os maiores importadores de soro de leite em pó (forma concentrada do soro de leite utilizada na alimentação, panificação, ração animal) visto que um grande volume da produção de queijos advém de pequenos produtores e, neste caso, o soro do leite não é reaproveitado para fins mais nobres. A Embrapa Agroindústria de Alimentos está coordenando um projeto internacional, com o financiamento da Agência Australiana para o Desenvolvimento Internacional, que busca ampliar a competitividade de pequenas queijarias a partir do reaproveitmento do soro do leite produzido. (GANDRA, 2013) Sansonetti et al. (2011) fizeram uso de um modelo estruturado para modelar a cinética da fermentação da lactose do soro de queijo sob condições anaeróbicas. Tal tipo de modelagem permite uma descrição mais fiel do sistema real devido à consideração de características específicas do meio e dos organismos presentes, visto que esse tipo de modelo é descrito com maior detalhamento, como por exemplo, pode abordar a concentração de DNA e de proteínas por unidade de peso seco de microorganismo. Esses modelos são mais robustos e atuam bem em situação onde modelos mais simplistas, como o de Monod, falham.
De acordo com Sansonetti et al. (2011), o modelo cinético utilizado para descrever o processo bioquímico pode ser descrito por um sistema de 4 equações diferenciais ordinárias, com 4 parâmetros desconhecidos: m, Kss, Ke e m, como mostrado nas Equações 5, 6, 7 e 8.

Cx: concentração de biomassa (Kluyveromyces marcianus) – g/L Cs: concentração de substrato (lactose) – g/L Cp1:concentração de etanol produzido – g/L Cp2: concentração de glicerol produzido – g/L
µm: Taxa específica de crescimento máxima td: Tempo de fase lag – horas Ke: Concentração de etanol que causa inibição - g/L Kss:Concentração de substrato limitante que causa inibição – g/L RODRIGUEZ et al. (2012) apresentaram um estudo fazendo uso da modelagem matemática com o objetivo de desenvolver uma plataforma de modelagem da dinâmica de produção de bioetanol a partir de material lignocelulósico, que permitisse uma simulação quantitativa e a comparação de diferentes configurações de processos para plantas de produção de bioetanol 2G.
Foi elaborado um modelo dinâmico para cada unidade de processo, e em seguida tudo foi conectado de modo a fornecer a modelagem dinâmica total da planta.
Embora interessante, os dados encontrados nesse material não estão de acordo com as necessidades do estudo desta Iniciação Científica, pois não fornecem os dados experimentais do mesmo, por isso, apesar de abordar a modelagem de um processo fermentativo para produção de um biocombustível, ele não foi utilizado.
A estimação de parâmetros em bioprocessos pode ser de extrema complexidade devido a uma possível alta correlação entre esses parâmetros, visto que muitas vezes os modelos apresentam equações hiperbólicas.

IV. METODOLOGIA
Tendo em vista o panorama geral do tema, a escolha do método de otimização a ser utilizado foi feita após a familiarização com os métodos de otimização existentes a partir de estudos, revisões bibliográficas e discussões com a orientadora. Foi definido a utilização do PSO como método de otimização, e a plataforma na qual seria desenvolvida a ferramenta como sendo o Visual Basic for Applications (VBA), devido à familiaridade da aluna com a mesma.
O programa desenvolvido pela aluna utilizando o PSO foi realizado com base em código Fortran desenvolvido previamente pela orientadora. Ao usuário é permitido a escolha dos parâmetros que vão direcionar a otimização (variáveis exigidas pelo PSO), como o número de partículas (é arbitrário, pode ser escolhido pelo usuário), número de variáveis que o PSO deve estimar (é fixo, pois eles estimará Kss, Ke, m e m) e número de iterações (é arbitrário, pode ser escolhido pelo usuário).
O número de partículas que compõem o enxame é arbitrário e , no caso deste trabalho de Iniciação Científica, foi deixado para ser escolhido pelo usuário da ferramenta, visto que um dos objetivos do projeto era fornecer uma maior interação do usuário com a ferramenta. (HERRERA, 2007; ESMIN, 2005).
A ferramenta desenvolvida está dividida em algumas subrotinas, apresentadas e descritas a seguir. a. Public Sub USUARIO_LB_UP()
Esta subrotina lê as informações inseridas pelo usuário na planilha para os parâmetros que vão direcionar a otimização: número de partículas, número de variáveis de busca para o PSO e número de iterações (variáveis exigidas pelo PSO). A partir desses valores, essa subrotina abre espaço na planilha para o preenchimento dos valores máximo e mínimo (upper e lower bound, UB e LB) que cada variável pode assumir. b. Public Sub Preenchimento_Dados_Experimentais()
O conjunto de dados experimentais que a ferramenta irá utilizar para a minimização do erro quadrático deve ser inserido pelo usuário. Primeiramente o usuário informa o número de produtos, substratos e pontos experimentais que o seu bioprocesso possui. Nesta subrotina, esses valores são lidos e a partir deles é montada na planilha uma tabela de dados experimentais, a ser preenchida pelo usuário.
Ao executar a subrotina, ela primeiramente limpa os dados ainda presentes na planilha advindos da otimização anterior. Para isso é utilizada a ferramenta do VBA
“ClearContents”.
Esta é a subrotina responsável pela dinamização da ferramenta no que consiste à inserção de dados experimentais, pelo usuário, de um processo biotecnológico de interesse, permitindo ao mesmo utilizar a ferramenta para sistemas com mais de um substrato no meio de cultivo, e que produzam mais de um produto.
Deve-se evidenciar que esta flexibilização foi feita para uso posterior, tendo em vista que, neste trabalho de Iniciação Científica, foi utilizado apenas o modelo estruturado de Sansonetti et al. (2011), em que o número de produtos e substratos já estavam definidos. Neste caso, por default, há dois produtos (glicerol e etanol), um substrato (lactose) e a biomassa foi a levedura Kluyveromyces marxianus. Com esta dinamização é montada a matriz para o preenchimento dos dados experimentais de acordo com os números de substratos, produtos e pontos experimentais que o usuário insere. c. Public Sub PSO()
Esta subrotina é a responsável pela execução da otimização. Nela estão contidos os códigos de leitura dos dados experimentais inseridos pelo usuário e do método de Otimização por Enxame de Partículas (PSO).
Os dados experimentais são lidos e armazenados em uma matriz A, cujos números de linhas e colunas dependem do número de pontos experimentais inseridos e do número de informações a serem inseridas (uma coluna refere-se aos dados da concentração de biomassa, e as outras dependem do número de substratos e produtos presentes no experimento), respectivamente.
Em seguida, as entradas da matriz A são normalizadas, ou seja, para cada coluna da matriz é identificado o maior valor e então todas as linhas são divididas por este valor. Isso deve ser feito pois cada componente do meio reacional (produtos, biomassa e substratos) tem uma ordem de grandeza de concentração diferente, o que pode comprometer o procedimento de otimização devido às contribuições diferentes que cada termo pode ter.
Feito isso, o próximo passo do programa é iniciar o processo iterativo de otimização. Como explicado, o PSO representa o comportamento de um grupo de pássaros, onde cada pássaro é uma partícula no espaço, e cada partícula é uma solução em potencial para o problema de otimização (no caso deste trabalho de Iniciação Científica trata-se de um problema de otimização de minimização do erro quadrático entre os pontos experimentais e os pontos estimados pelo modelo de modo a encontrar o melhor conjunto de parâmetros que descrevem o sistema).Cada partícula possui uma coordenada no espaço, ou seja, possui variáveis (parâmetros do modelo, no caso) que a localizam no espaço. Por se tratar de um método de otimização estocástico, ele se baseia no sorteio de números aleatórios ao longo da otimização, a cada iteração (“k”). Portanto, os números aleatórios são sorteados e determinam a localização inicial das partículas no espaço.
Dentro de cada iteração (“k”), para cada partícula é calculado o valor da função objetivo que ela assume, dada a sua posição no espaço. Para tal, é chamada uma quarta subrotina (chamada de PSO2()). Estando na PSO2(), as coordenadas da partícula definem o conjunto de parâmetros (Kss, Ke, m e m), e a primeira linha da matrix A define os valores iniciais a serem utilizados na integração numérica do modelo cinético, formado pelo conjunto de equações diferenciais ordinárias (EDOs), Equações 5-8. Neste modelo cinético um parâmetros que não foi ajustado foi o td (tempo de fase lag). Para tal parâmetro foi utilizado o valor de 1,9 horas, segundo Sansonetti et al. (2011). A solução do sistema de equações diferenciais apresentadas como modelo cinético do sistema (Equações 5 a 8) se deu com a utilização do método de integração numérica conhecido por Runge-Kutta (RK). O mais conhecido, Runge-Kutta de 4ª ordem, fornece solução mais aproximada da realidade. Ele se baseia no polinômio de Taylor de 4º grau, e consiste em encontrar as constantes da Equação 9 de tal forma que ela coincida com este polinômio, Equação 10, para o caso de uma única equação diferencial a ser resolvida.
Como se trata de um sistema de 4 equações, Equações 5 a 8, consideram-se as funções descritas pelas Equações 11 a 14.
As soluções para as Equações 11 a 14 podem ser obtidas pelo método do Runge Kutta de 4ª ordem usando as Equações 15 a 34.
Com a solução deste sistema de equações diferenciais pelo método apresentado foi estimado o comportamento das concentrações de substrato, produto e biomassa ao longo do tempo para cada partícula na iteração “k”. Estes valores estimados são armazenados em uma matriz, chamada de “matrizmodelo”, que também foi normalizada pelo mesmo motivo da matriz A explicado anteriormente, obtendo-se assim a “matrizmodelonorm”.
Para encontrar o valor da função objetivo para cada partícula, são utilizados os dados experimentais (matriz A) e os dados estimados (“matrizmodelonorm”). A função objetivo de minimização do erro quadrático para o modelo estruturado, apresentado por Sansonetti et al. (2011), é então calculada segundo a Equação 35.

(35) Onde: f: função objetivo de minimização do erro quadrático; n: número de variáveis (substrato, produtos, células); m: número de pontos experimentais ( ): valor da variável “n” estimado pelo modelo no tempo “t”; ( ): valor da variável “n” experimental, inserido pelo usuário, no tempo “t”.
Assim, para cada partícula é calculado o valor da função objetivo “f” na subrotina PSO2() e esse valor é retornado para a subrotina principal PSO().
Na subrotina principal, os valores da função objetivo de cada partícula são verificados localmente e globalmente. Para cada partícula é verificado se aquele valor de “f” da iteração em que ela se encontra é melhor ou pior do que o melhor que ela já possuiu (o melhor que ela já possuiu é chamado de melhor local para a partícula “j”).
Se for melhor, esse novo valor de “f” é armazenado, se não, o valor anterior permanece armazenado. Tendo feito essa análise para todas as partículas, cada partícula possui, neste momento, o seu melhor local definido dentre todas as iterações até o momento (iteração “k”). Então é identificado qual o melhor global de todo o enxame (o menor valor dentre os melhores locais de cada partícula). Esse melhor global é representado por uma partícula cujas coordenadas representam o melhor conjunto de parâmetros cinéticos para o sistema (Kss, Ke, m e m) até essa iteração “k”.
Tendo definido os ótimos locais e o ótimo global da iteração “k”, este último é encaminhado para a subrotina PSO2() de modo a calcular o valor ótimo dos parâmetros, e em seguida são gerados os gráficos, na planilha, do perfil de concentração de etanol, glicerol, biomassa e lactose estimados pelo modelo. Para a geração desses gráficos o VBA “escreve” em uma região determinada da planilha a tabela referente a esse perfil de concentração referente ao menor erro quadrático encontrado até o momento. A partir desta tabela e da tabela de dados experimentais fornecidos pelo usuário são plotados os gráficos comparativos apresentados nas Figuras 13, 14, 15 e 16. A posição das partículas para a iteração seguinte (“k+1”)
(Equação 1) é calculada a partir de um novo sorteio de números aleatórios que irão ser utilizados no cálculo do vetor velocidade (Equação 2) que indica a direção com que a partícula irá se movimentar. Sua nova posição para a iteração “k+1” é a posição em que ela se encontra na iteração “k” somada com o vetor velocidade.
Sendo assim, esse mesmo processo é repetido a cada iteração. A otimização é encerrada quando é realizada a última iteração, e o mínimo global é identificado.
Nas Figuras 7 e 8 são apresentados diagramas do funcionamento do programa. Na Figura 7 é feito um panorama geral das subrotinas abordadas nos tópicos a, b e c.
Na Figura 8 é apresentado o fluxograma do PSO.



V. RESULTADOS E DISCUSSÃO
Tendo em vista a descrição da ferramenta, apresentada na Figura 8, com o método de otimização por Enxame de Partículas, fez parte do trabalho a validação da ferramenta, ou seja, verificação dos resultados fornecidos pelo modelo utilizado na ferramenta de acordo com dados experimentais.
Para que fosse possível esta validação utilizaram-se dados experimentais encontrados em Sansonetti et al. (2011). Tais dados foram retirados de gráficos que são apresentados no artigo. Sansonetti et al. (2011) realizaram três corridas de experimentos, e para cada uma delas encontrou-se um perfil de concentração de lactose, etanol, glicerol e biomassa ao longo do tempo (Figura 9, Figura 10). Destes gráficos foram coletados os pontos experimentais apresentados na Tabela 1 e cada concentração ao longo do tempo. Essa média foi a utilizada na validação da ferramenta, apresentada na Tabela 3.




De posse dos dados experimentais foi realizada uma estimativa de parâmetros com a ferramenta. Nas Figura 11 e Figura 12 são apresentadas as etapas de preenchimento de inputs que o usuário deve cumprir para realizar a estimativa dos parâmetros. Os passos 1, 2 e 3 devem ser realizados nesta ordem, e em seguida é gerada a tabela para preenchimento dos dados experimentais, apresentada na
Fonte: Acervo Pessoal Neste caso aparecem as células já preenchidas para a validação, com 4 partículas, 4 variáveis (são quatro parâmetros a serem estimados, Kss, Ke, m e m) e o número de iterações foi escolhido de modo que a simulação não demorasse muito tempo para rodar e fornecesse bons resultados, assim como as células para definição do Lower e Upper Bound das variáveis (dos parâmetros Kss, Ke, m e m).
Tais dados devem ser preenchidos pelo usuário de acordo com sua experiência e expectativa quanto aos valores dos parâmetros a serem estimados. Neste caso, os valores apresentados na Figura 11 são muito próximos do resultado esperado para verificar qual era o comportamento dos pontos do modelo quando comparados com os experimentais.

Figura 12: Tabela gerada com base nos passos 1, 2 e 3 para preenchimento dos dados experimentais Fonte: Acervo Pessoal Neste caso aparecem 16 linhas visto que para o caso de validação foram utilizados os pontos da Tabela 3. Tais pontos foram preenchidos na tabela da Figura 12, e o botão “Executar Otimização” foi apertado.
Para o conjunto de inputs colocados na ferramenta, o resultado obtido está apresentado na Tabela 4.
Este conjunto de parâmetros obtidos na estimação de parâmetros reflete os perfis de concentração ao longo do tempo como mostrado nas Figuras 13, 14 15 e
16.


Pôde-se perceber pela análise dos gráficos e dos resultados para os valores dos parâmetros que a ferramenta pode ser considerada validada, dado que os desvios apresentados são satisfatórios, tendo em vista que se trata de um processo biológico complexo, por apresentar inibições por produto e substrato, fase lag.
Entretanto, vale ressaltar que a ferramenta desenvolvida utiliza um método de otimização estocástico, isso significa que os resultados obtidos não serão os mesmos ainda que os inputs não sejam alterados, pois como comentado, ele faz uso do sorteio de números aleatórios ao longo da execução da otimização.
Realizando alterações nos inputs de Lower e Upper Bounds, número de iterações e número de partículas (visto que os demais são fixos para cada tipo diferente de experimento), os resultados para os valores dos parâmetros também são alterados. A Tabela 5 apresenta os resultados para as variações.

Pôde-se observar que à medida em que o intervalo de Lower e Upper Bounds aumentam (Casos 2 e 3) com relação ao caso base (Caso 1), mantendo os números de iterações e partículas constantes, os valores dos parâmetros estimados pela ferramenta se distanciam mais do esperado apresentado em Sansonetti et al. (2011), como visto na última coluna da Tabela 5, bem como os valores do erro quadrático aumentaram, o que indica que o conjunto estimado de parâmetros piorou com o aumento dos intervalos de busca, sem estar atrelado a um aumento no número de iterações ou do número de partículas.
Quanto à variação no número de iterações, nada se pôde concluir quando observado os desvios com relação a Sansonetti et al. (2011), visto que o comportamento é aparentemente aleatório quando verificado o os desvios dos casos 4, 5 e 6. Entretanto, percebeu-se que um maior número de iterações forneceu um menor desvio quadrático (caso 6 em comparação aos casos 4 e 5) o que pode indicar um melhor conjunto de parâmetros que os encontrados por Sansonetti et al.
(2011).
É válido ressaltar que os parâmetros de Sansonetti et al. (2011) não são, necessariamente, os melhores parâmetros para descrever a cinética do processo abordado, haja vista que no caso de se ter um desvio maior mas um erro quadrático menor é um indicativo de que o conjunto de parâmetros encontrados pode ser melhor com relação a Sansonetti et al. (2011).
Já para a variação no número de partículas da otimização, casos 7, 8 e 9, o aumento no número de partículas gerou uma redução no erro quadrático quando comparados os casos 7 e 8, e os casos 7 e 9. Porém, quando comparados os casos 8 e 9 o aumento do número de partículas ocasionou um pequeno aumento no valor do erro quadrático. Esses comportamentos ocorrem porque se trata de um processo de otimização aleatório, ou seja, não existe um padrão bem definido do comportamento dos resultados. O que pode-se afirmar é que a tendência dos resultados é a minimização, porém com pequenas oscilações devido à presença do fator de aleatoriedade. Espera-se, no entanto, que um maior número de partículas e um maior número de iterações possibilite a obtenção de menores valores de erro quadrático, visto que se tende a aumentar o número de posições visitadas pelo enxame de partículas.
Os resultados mostram que a ferramenta desenvolvida permite que o usuário faça a estimativa de parâmetros cinéticos de modelo de bioprocesso sem que seja necessário conhecimento acerca de programação matemática. Entretanto, observou-se que o usuário da ferramenta deve ter expectativas sobre a ordem de grandeza que espera para os parâmetros de modo que forneça limites inferiores e superiores coerentes para os mesmos. Foi verificado que intervalos grandes para esses limites causam consideráveis desvios nos valores dos parâmetros quando comparados com o de Sansonetti et al (2011), bem como valores de erros quadráticos maiores, como observado nos ensaios 1, 2 e 3 (mas a utilização de número maior de partículas e de iterações pode neutralizar a maior dificuldade em se chegar a um bom conjunto de parâmetros) . A ferramenta é dinâmica, permitindo que o usuário varie parâmetros do algoritmo de otimização e verifique os parâmetros estimados e, tanto em número (valor do erro quadrático) quanto visualmente (através dos gráficos que mostram dados experimentais com valores calculados pelo modelo), possa avaliar a qualidade do ajuste paramétrico feito.

VI. CONCLUSÃO
Diante do cenário de elevado consumo energético, e tendência de crescimento do mesmo, as energias renováveis são alternativas para que a ampliação deste consumo ocorra causando menos agressão ao meio ambiente, por não alterarem o balanço de carbono do ambiente. É nesse sentido que estudos sobre a produção de biocombustíveis têm sido realizados com o objetivo de colocá-los frente a frente em nível de competitividade com os combustíveis fósseis. Porém, trata-se de processos biológicos cuja modelagem é difícil de ser realizada devido à presença de muitas fontes de variação.
No contexto de modelagem se insere a estimação de parâmetros do modelo. Em geral, pesquisadores que realizam cultivos experimentais são pouco familiarizados com métodos matemáticos de estimativa de parâmetros, e há grande deficiência em ferramentas amigáveis para auxílio nesse ramo de pesquisa. Portanto, motivado por esta realidade, elaborou-se uma ferramenta em Visual Basic for Applications (VBA) utilizando o Enxame de Partículas (PSO) como método de otimização.
A ferramenta desenvolvida se mostrou adequada para a estimativa de parâmetros de bioprocessos, e também se mostrou amigável ao usuário, visto que não exige do mesmo domínio de métodos de otimização.
Foram utilizados dados experimentais de Sansonetti et al. (2011), de produção de etanol a partir de soro de leite, para a validação da ferramenta, que apresentou bons resultados para os parâmetros cinéticos estimados, com baixo valor de erro quadrático.
Pelo método de otimização pertencer à família dos métodos estocásticos, o resultado encontrado para os parâmetros estimados não foi o mesmo nem em casos em que nenhum input é modificado. Apenas por realizar a estimativa duas vezes na ferramenta o resultado foi diferente. Entretanto, apresentou poucas variações.
Notou-se que é necessário que o usuário tenha experiência em processos biológicos para que os intervalos de Lower e Upper Bounds inseridos configurem um intervalo razoável para a estimativa dos parâmetros. Ou seja, lidar com estimativa de parâmetros de bioprocessos exige, ainda que com o auxílio de uma ferramenta computacional, experiência do usuário quanto ao que deve ser esperado.
