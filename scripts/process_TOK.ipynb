{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600694375954",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import estrutura_ud\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "build: 171.933739900589build: 402.37909984588623build: 1739.2451667785645build: 3781.8036665916443build: 174.64445972442627build: 2830.0068171024323\naline\nSentenças com tokenização diferente: 58\nNovas sentenças: 117\nSentenças deletadas: 123\n\nelvis\nSentenças com tokenização diferente: 58\nNovas sentenças: 37\nSentenças deletadas: 28\n\ntati\nSentenças com tokenização diferente: 103\nNovas sentenças: 78\nSentenças deletadas: 54\n"
    }
   ],
   "source": [
    "# Comparar diferenças\n",
    "names = ['aline', 'elvis', 'tati']\n",
    "\n",
    "for name in names:\n",
    "    os.system(\"wget http://interrogatorio.ica.ele.puc-rio.br/interrogar-ud/conllu/TOK_{0}.conllu -O ../TOK/TOK_{0}_golden.conllu\".format(name.title()))\n",
    "\n",
    "system = {}\n",
    "revised = {}\n",
    "\n",
    "for name in names:\n",
    "    system[name] = estrutura_ud.Corpus(recursivo=False)\n",
    "    revised[name] = estrutura_ud.Corpus(recursivo=False)\n",
    "    system[name].load('../TOK/TOK_{}.conllu'.format(name.title()))\n",
    "    revised[name].load('../TOK/TOK_golden.conllu'.format(name.title()))\n",
    "\n",
    "append_to_golden = lambda name, sent_id: golden.sentences.update({sent_id: copy.deepcopy(revised[name].sentences[sent_id])})\n",
    "golden = estrutura_ud.Corpus()\n",
    "different_tokenization = {}\n",
    "new_sentences = {}\n",
    "deleted_sentences = {}\n",
    "\n",
    "for name in names:\n",
    "    different_tokenization[name] = []\n",
    "    new_sentences[name] = []\n",
    "    deleted_sentences[name] = []\n",
    "    new_sentences[name] = set(revised[name].sentences.keys()) - set(system[name].sentences.keys())\n",
    "    deleted_sentences[name] = set(system[name].sentences.keys()) - set(revised[name].sentences.keys())\n",
    "    for sent_id in revised[name].sentences:\n",
    "        if sent_id in system[name].sentences:\n",
    "            if revised[name].sentences[sent_id].to_str() != system[name].sentences[sent_id].to_str():\n",
    "                append_to_golden(name, sent_id)\n",
    "                different_tokenization[name].append(sent_id)\n",
    "        else:\n",
    "            append_to_golden(name, sent_id)\n",
    "    \n",
    "    print(\"\\n{}\\nSentenças com tokenização diferente: {}\\nNovas sentenças: {}\\nSentenças deletadas: {}\".format(\n",
    "        name,\n",
    "        len(different_tokenization[name]),\n",
    "        len(new_sentences[name]),\n",
    "        len(deleted_sentences[name])\n",
    "    ))\n",
    "    \n",
    "for sent_id in golden.sentences:\n",
    "    for token in golden.sentences[sent_id].tokens:\n",
    "        token.lemma = \"_\"\n",
    "        token.upos = \"_\"\n",
    "        token.xpos = \"_\"\n",
    "        token.feats = \"_\"\n",
    "        token.dephead = \"_\"\n",
    "        token.deprel = \"_\"\n",
    "        token.deps = \"_\"\n",
    "        token.misc = \"_\"\n",
    "\n",
    "previous_golden = estrutura_ud.Corpus()\n",
    "previous_golden.load(\"../TOK/TOK_golden.conllu\")\n",
    "for sentence in previous_golden.sentences:\n",
    "    if sentence not in golden.sentences:\n",
    "        golden.sentences[sentence] = copy.deepcopy(previous_golden.sentences[sentence])\n",
    "golden.save(\"../TOK/TOK_2_golden.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}