{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599254662755",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import os\n",
    "import estrutura_ud\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = ['aline', 'elvis', 'tatiana']\n",
    "\n",
    "system = defaultdict(estrutura_ud.Corpus)\n",
    "revised = defaultdict(estrutura_ud.Corpus)\n",
    "golden = {}\n",
    "different_tokenization = {}\n",
    "new_sentences = {}\n",
    "deleted_sentences = {}\n",
    "\n",
    "for name in names:\n",
    "    system[name].load('../TOK/TOK_{}.conllu'.format(name.title()))\n",
    "    revised[name].load('../TOK/TOK_{}_golden.conllu'.format(name.title()))\n",
    "    golden[name] = estrutura_ud.Corpus()\n",
    "    different_tokenization[name] = []\n",
    "    new_sentences[name] = []\n",
    "    deleted_sentences[name] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_golden = lambda name, sent_id: golden[name].sentences.update({sent_id: revised[name].sentences[sent_id]})\n",
    "\n",
    "for name in names:\n",
    "    new_sentences[name] = set(revised[name].sentences.keys()) - set(system[name].sentences.keys())\n",
    "    deleted_sentences[name] = set(system[name].sentences.keys()) - set(revised[name].sentences.keys())\n",
    "    for sent_id in revised[name].sentences:\n",
    "        if sent_id in system[name]:\n",
    "            if revised[name].sentences[sent_id] != system[name].sentences[sent_id]:\n",
    "                append_to_golden(name, sent_id)\n",
    "        else:\n",
    "            append_to_golden(name, sent_id)\n",
    "    \n",
    "    print(\"{}\\nSentenças com tokenização diferente: {}\\nNovas sentenças: {}\\nSentenças deletadas: {}\\n\".format(\n",
    "        name,\n",
    "        len(different_tokenization[name]),\n",
    "        len(new_sentences[name]),\n",
    "        len(deleted_sentences[name])\n",
    "    ))\n",
    "            "
   ]
  }
 ]
}